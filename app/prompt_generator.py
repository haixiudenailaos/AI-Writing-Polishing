"""
æç¤ºè¯ç”Ÿæˆå™¨
åŸºäºçŸ¥è¯†åº“æ–‡æ¡£å†…å®¹è‡ªåŠ¨ç”Ÿæˆå®šåˆ¶åŒ–çš„æ¶¦è‰²é£æ ¼æç¤ºè¯å’Œé¢„æµ‹æç¤ºè¯
"""

import random
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter
import re


class PromptGenerator:
    """æç¤ºè¯ç”Ÿæˆå™¨ - ä»çŸ¥è¯†åº“å†…å®¹ä¸­æå–ç‰¹å¾å¹¶ç”Ÿæˆæç¤ºè¯"""
    
    def __init__(self):
        """åˆå§‹åŒ–æç¤ºè¯ç”Ÿæˆå™¨"""
        pass
    
    def extract_features_from_documents(self, documents: List[Any], sample_size: int = 50) -> Dict[str, Any]:
        """ä»æ–‡æ¡£ä¸­æå–ç‰¹å¾
        
        Args:
            documents: çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨
            sample_size: é‡‡æ ·æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤50ä¸ªï¼‰
            
        Returns:
            ç‰¹å¾å­—å…¸ï¼ŒåŒ…å«ï¼š
            - avg_sentence_length: å¹³å‡å¥å­é•¿åº¦
            - punctuation_freq: æ ‡ç‚¹ç¬¦å·ä½¿ç”¨é¢‘ç‡
            - vocabulary_richness: è¯æ±‡ä¸°å¯Œåº¦
            - common_phrases: å¸¸è§çŸ­è¯­
            - writing_style: å†™ä½œé£æ ¼ç‰¹å¾
            - common_patterns: å¸¸è§å¥å¼æ¨¡å¼
        """
        if not documents:
            return self._get_default_features()
        
        # éšæœºé‡‡æ ·æ–‡æ¡£ï¼ˆé¿å…å¤„ç†å…¨éƒ¨æ–‡æ¡£ï¼Œæé«˜æ•ˆç‡ï¼‰
        sampled_docs = random.sample(documents, min(sample_size, len(documents)))
        
        # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
        texts = [doc.content for doc in sampled_docs]
        full_text = "\n".join(texts)
        
        features = {}
        
        # 1. åˆ†æå¥å­é•¿åº¦
        sentences = self._split_sentences(full_text)
        if sentences:
            sentence_lengths = [len(s) for s in sentences]
            features['avg_sentence_length'] = sum(sentence_lengths) / len(sentence_lengths)
            features['sentence_length_variance'] = self._calculate_variance(sentence_lengths)
        else:
            features['avg_sentence_length'] = 20
            features['sentence_length_variance'] = 0
        
        # 2. åˆ†ææ ‡ç‚¹ç¬¦å·ä½¿ç”¨
        features['punctuation_freq'] = self._analyze_punctuation(full_text)
        
        # 3. åˆ†æè¯æ±‡ç‰¹å¾
        features['vocabulary_richness'] = self._analyze_vocabulary(full_text)
        
        # 4. æå–å¸¸è§çŸ­è¯­
        features['common_phrases'] = self._extract_common_phrases(texts, top_n=10)
        
        # 5. åˆ†æå†™ä½œé£æ ¼
        features['writing_style'] = self._analyze_writing_style(full_text, features)
        
        # 6. æå–å¥å¼æ¨¡å¼
        features['common_patterns'] = self._extract_sentence_patterns(sentences, top_n=5)
        
        return features
    
    def generate_polish_style_prompt(self, kb_name: str, features: Dict[str, Any]) -> str:
        """åŸºäºç‰¹å¾ç”Ÿæˆæ¶¦è‰²é£æ ¼æç¤ºè¯
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            features: æ–‡æ¡£ç‰¹å¾
            
        Returns:
            æ¶¦è‰²é£æ ¼æç¤ºè¯
        """
        style = features.get('writing_style', {})
        avg_length = features.get('avg_sentence_length', 20)
        punct_freq = features.get('punctuation_freq', {})
        
        prompt_parts = [
            f"ã€{kb_name}é£æ ¼ã€‘",
            ""
        ]
        
        # 1. å¥å­é•¿åº¦é£æ ¼
        if avg_length < 15:
            prompt_parts.append("- ä½¿ç”¨ç®€çŸ­ç²¾ç‚¼çš„å¥å­ï¼Œæ¯å¥æ§åˆ¶åœ¨15å­—å·¦å³")
        elif avg_length > 30:
            prompt_parts.append("- ä½¿ç”¨è¾ƒé•¿çš„å¥å­ç»“æ„ï¼Œæ³¨é‡ç»†èŠ‚æå†™å’Œå±‚æ¬¡æ„Ÿ")
        else:
            prompt_parts.append("- ä¿æŒå¥å­é•¿åº¦é€‚ä¸­ï¼Œæ—¢ä¸è¿‡äºç®€çŸ­ä¹Ÿä¸è¿‡äºå†—é•¿")
        
        # 2. å¯¹è¯é£æ ¼
        if style.get('has_dialogue', False):
            prompt_parts.append("- åŒ…å«å¯¹è¯æ—¶ï¼Œæ³¨é‡å¯¹è¯çš„ç”ŸåŠ¨æ€§å’Œäººç‰©æ€§æ ¼å±•ç°")
        
        # 3. æå†™é£æ ¼
        if style.get('descriptive_level') == 'high':
            prompt_parts.append("- æ³¨é‡ç»†è…»çš„åœºæ™¯å’Œå¿ƒç†æå†™ï¼Œè¥é€ æ°›å›´æ„Ÿ")
        elif style.get('descriptive_level') == 'low':
            prompt_parts.append("- ä»¥æƒ…èŠ‚æ¨è¿›ä¸ºä¸»ï¼Œæå†™ç®€æ´æ˜å¿«")
        
        # 4. æƒ…æ„ŸåŸºè°ƒ
        emotion_tone = style.get('emotion_tone', 'neutral')
        if emotion_tone == 'positive':
            prompt_parts.append("- ä¿æŒç§¯æå‘ä¸Šçš„æƒ…æ„ŸåŸºè°ƒ")
        elif emotion_tone == 'melancholic':
            prompt_parts.append("- ä¿æŒå«è“„æ·±æ²‰çš„æƒ…æ„Ÿè¡¨è¾¾")
        
        # 5. æ ‡ç‚¹ç¬¦å·ä½¿ç”¨åå¥½
        if punct_freq.get('ellipsis', 0) > 0.02:
            prompt_parts.append("- é€‚å½“ä½¿ç”¨çœç•¥å·è¡¨è¾¾æœªå°½ä¹‹æ„")
        if punct_freq.get('exclamation', 0) > 0.03:
            prompt_parts.append("- ä½¿ç”¨æ„Ÿå¹å·å¼ºåŒ–æƒ…æ„Ÿè¡¨è¾¾")
        
        # 6. è¯æ±‡ç‰¹ç‚¹
        vocab_richness = features.get('vocabulary_richness', 0)
        if vocab_richness > 0.7:
            prompt_parts.append("- ä½¿ç”¨ä¸°å¯Œå¤šæ ·çš„è¯æ±‡ï¼Œé¿å…é‡å¤")
        elif vocab_richness < 0.4:
            prompt_parts.append("- ä½¿ç”¨ç®€æ´å¸¸è§çš„è¯æ±‡ï¼Œç¡®ä¿é€šä¿—æ˜“æ‡‚")
        
        # 7. æ·»åŠ é€šç”¨æ¶¦è‰²è¦æ±‚
        prompt_parts.extend([
            "",
            "ã€é€šç”¨è¦æ±‚ã€‘",
            "- ä¿æŒåŸæ–‡çš„æ ¸å¿ƒæ„æ€å’Œæƒ…èŠ‚å‘å±•",
            "- ä¼˜åŒ–è¯­å¥é€šé¡ºåº¦å’Œå¯è¯»æ€§",
            "- ä¿æŒä¸åŸæ–‡é£æ ¼çš„ä¸€è‡´æ€§"
        ])
        
        return "\n".join(prompt_parts)
    
    def generate_prediction_prompt(self, kb_name: str, features: Dict[str, Any]) -> str:
        """åŸºäºç‰¹å¾ç”Ÿæˆé¢„æµ‹æç¤ºè¯
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
            features: æ–‡æ¡£ç‰¹å¾
            
        Returns:
            é¢„æµ‹æç¤ºè¯ï¼ˆæ³¨é‡åˆ›æ„å¯å‘è€Œéç®€å•æ¨¡ä»¿ï¼‰
        """
        style = features.get('writing_style', {})
        common_patterns = features.get('common_patterns', [])
        avg_length = features.get('avg_sentence_length', 20)
        
        prompt_parts = [
            f"ã€{kb_name}åˆ›æ„å‰§æƒ…é¢„æµ‹ã€‘",
            "",
            "ä½ çš„æ ¸å¿ƒä»»åŠ¡ï¼šä¸ºåˆ›ä½œè€…æä¾›ã€Œæ–°é¢–ã€æœ‰å¼ åŠ›ã€å‡ºäººæ„æ–™ä½†åˆç†ã€çš„å‰§æƒ…æ€è·¯ã€‚",
            ""
        ]
        
        # ===== ç¬¬ä¸€éƒ¨åˆ†ï¼šåˆ›æ„å¼•å¯¼åŸåˆ™ =====
        prompt_parts.extend([
            "ã€åˆ›æ„å¼•å¯¼åŸåˆ™ã€‘",
            "1. ğŸ­ åˆ¶é€ æ„å¤–è½¬æŠ˜ï¼šåœ¨è¯»è€…é¢„æœŸä¹‹å¤–ï¼Œä½†äº‹åç»†æƒ³ç¬¦åˆé€»è¾‘",
            "2. ğŸ’¥ å¢å¼ºæˆå‰§å†²çªï¼šæŒ–æ˜äººç‰©çŸ›ç›¾ã€ç›®æ ‡éšœç¢ã€ä»·å€¼è§‚ç¢°æ’",
            "3. ğŸ” æ·±æŒ–éšè—çº¿ç´¢ï¼šå‘ç°æ–‡æœ¬ä¸­æœªæ˜è¯´çš„ä¼ç¬”ã€åŠ¨æœºã€æ½œå°è¯",
            "4. ğŸ¯ å¼ºåŒ–äººç‰©åŠ¨æœºï¼šè®©è§’è‰²è¡Œä¸ºæºäºæ·±å±‚æ¬²æœ›ã€ææƒ§æˆ–ç§˜å¯†",
            "5. ğŸŒŠ è¥é€ æƒ…æ„Ÿå¼ åŠ›ï¼šé€šè¿‡å¯¹æ¯”ã€åå·®ã€å»¶è¿Ÿæ»¡è¶³å¢å¼ºæ„ŸæŸ“åŠ›",
            "6. ğŸ§© åŸ‹è®¾æ‚¬å¿µç§å­ï¼šä¸ºåç»­æƒ…èŠ‚ç•™ä¸‹é’©å­ï¼Œå¼•å‘è¯»è€…å¥½å¥‡",
            ""
        ])
        
        # ===== ç¬¬äºŒéƒ¨åˆ†ï¼šåˆ›æ„æŠ€å·§å·¥å…·ç®± =====
        prompt_parts.extend([
            "ã€åˆ›æ„æŠ€å·§å·¥å…·ç®±ã€‘",
            "â€¢ è§†è§’è½¬æ¢ï¼šåˆ‡æ¢åˆ°æ¬¡è¦è§’è‰²æˆ–å¯¹ç«‹æ–¹çš„è§†è§’",
            "â€¢ æ—¶é—´è·³è·ƒï¼šé—ªå›å…³é”®è®°å¿†ï¼Œæˆ–å¿«è¿›åˆ°å…³é”®æ—¶åˆ»",
            "â€¢ æƒ…ç»ªåè½¬ï¼šä»é«˜æ½®åˆ°ä½è°·ï¼Œæˆ–ä»ç»æœ›åˆ°å¸Œæœ›",
            "â€¢ ä¿¡æ¯ä¸å¯¹ç§°ï¼šè®©è§’è‰²çŸ¥é“è¯»è€…ä¸çŸ¥é“çš„ï¼Œæˆ–åä¹‹",
            "â€¢ é“å…·/ç»†èŠ‚ï¼šç”¨çœ‹ä¼¼æ— å…³çš„å°ç‰©ä»¶è§¦å‘å¤§äº‹ä»¶",
            "â€¢ ç¯å¢ƒå¹²é¢„ï¼šå¤©æ°”ã€åœºæ‰€ã€æ„å¤–äº‹ä»¶æ‰“æ–­åŸè®¡åˆ’",
            "â€¢ å¯¹è¯æš—æµï¼šè¡¨é¢å¹³é™çš„å¯¹è¯ä¸‹æš—è—é”‹èŠ’",
            ""
        ])
        
        # ===== ç¬¬ä¸‰éƒ¨åˆ†ï¼šé£æ ¼é€‚é… =====
        prompt_parts.append("ã€é£æ ¼é€‚é…è¦æ±‚ã€‘")
        
        # å™äº‹è§†è§’
        narrative_perspective = style.get('narrative_perspective', 'third')
        if narrative_perspective == 'first':
            prompt_parts.append("âœ“ ç¬¬ä¸€äººç§°å™è¿°ï¼šæ·±å…¥ä¸»è§’å†…å¿ƒï¼Œå±•ç°ä¸»è§‚æ„Ÿå—ä¸è¯¯åˆ¤")
        else:
            prompt_parts.append("âœ“ ç¬¬ä¸‰äººç§°å™è¿°ï¼šå¯å±•ç°å¤šè§’è‰²è§†è§’ï¼Œæ­ç¤ºæ›´å¤šä¿¡æ¯å±‚æ¬¡")
        
        # èŠ‚å¥æ§åˆ¶
        pacing = style.get('pacing', 'medium')
        if pacing == 'fast':
            prompt_parts.append("âœ“ å¿«èŠ‚å¥ï¼šç”¨çªå‘äº‹ä»¶ã€å¿«é€Ÿå¯¹è¯ã€è¿ç¯åŠ¨ä½œæ¨è¿›")
        elif pacing == 'slow':
            prompt_parts.append("âœ“ æ…¢èŠ‚å¥ï¼šç”¨ç»†è…»å¿ƒç†ã€ç¯å¢ƒæå†™ã€æƒ…ç»ªé“ºå«è¥é€ æ°›å›´")
        else:
            prompt_parts.append("âœ“ é€‚ä¸­èŠ‚å¥ï¼šå¼ å¼›æœ‰åº¦ï¼Œåœ¨å…³é”®å¤„åŠ é€Ÿæˆ–å‡é€Ÿ")
        
        # å¥å­é•¿åº¦é£æ ¼
        if avg_length < 15:
            prompt_parts.append("âœ“ çŸ­å¥é£æ ¼ï¼šç”¨å¹²è„†åˆ©è½çš„å¥å­åˆ¶é€ ç´§å¼ æ„Ÿ")
        elif avg_length > 30:
            prompt_parts.append("âœ“ é•¿å¥é£æ ¼ï¼šç”¨å¤æ‚å¥å¼å±•ç°å±‚æ¬¡æ„Ÿå’Œæ€è€ƒæ·±åº¦")
        else:
            prompt_parts.append("âœ“ å¥å¼çµæ´»ï¼šæ ¹æ®æƒ…èŠ‚éœ€è¦è°ƒæ•´é•¿çŸ­")
        
        # å¯¹è¯ä½¿ç”¨
        if style.get('has_dialogue', False):
            prompt_parts.append("âœ“ å–„ç”¨å¯¹è¯ï¼šè®©å¯¹è¯æ¨åŠ¨æƒ…èŠ‚ã€æ­ç¤ºæ€§æ ¼ã€åˆ¶é€ å†²çª")
        
        # æƒ…æ„ŸåŸºè°ƒ
        emotion_tone = style.get('emotion_tone', 'neutral')
        if emotion_tone == 'positive':
            prompt_parts.append("âœ“ åŸºè°ƒåå‘ï¼šåœ¨ç§¯æä¸­åŸ‹è®¾éšå¿§ï¼Œå¢åŠ æˆå‰§å¼ åŠ›")
        elif emotion_tone == 'melancholic':
            prompt_parts.append("âœ“ åŸºè°ƒåå‘ï¼šåœ¨å¿§éƒä¸­å¯»æ‰¾å¸Œæœ›ç«å…‰ï¼Œå½¢æˆåå·®")
        else:
            prompt_parts.append("âœ“ æƒ…æ„Ÿçµæ´»ï¼šæ ¹æ®å‰§æƒ…éœ€è¦è°ƒé…æƒ…ç»ªè‰²å½©")
        
        prompt_parts.append("")
        
        # ===== ç¬¬å››éƒ¨åˆ†ï¼šå¥å¼å‚è€ƒï¼ˆå¯é€‰ï¼‰ =====
        if common_patterns:
            prompt_parts.append("ã€å¥å¼èŠ‚å¥å‚è€ƒã€‘")
            for pattern in common_patterns[:3]:
                prompt_parts.append(f"â€¢ {pattern}ï¼ˆå¯åˆ›é€ æ€§å˜åŒ–ï¼‰")
            prompt_parts.append("")
        
        # ===== ç¬¬äº”éƒ¨åˆ†ï¼šæ ¸å¿ƒè¾“å‡ºè¦æ±‚ =====
        prompt_parts.extend([
            "ã€æ ¸å¿ƒè¾“å‡ºè¦æ±‚ã€‘",
            "âœ¦ ä¼˜å…ˆçº§æ’åºï¼šåˆ›æ„æ–°é¢–åº¦ > æˆå‰§å¼ åŠ› > é£æ ¼å¥‘åˆåº¦ > è¯­è¨€æµç•…åº¦",
            "âœ¦ æ€è€ƒè·¯å¾„ï¼š",
            "  1) åˆ†æå½“å‰æƒ…å¢ƒçš„æ½œåœ¨å†²çªç‚¹å’Œè½¬æŠ˜å¯èƒ½",
            "  2) è¯†åˆ«æœ€æ„æƒ³ä¸åˆ°ä½†åˆæš—è—ä¼ç¬”çš„å‘å±•æ–¹å‘",
            "  3) é€‰æ‹©èƒ½æœ€å¤§åŒ–æƒ…æ„Ÿå†²å‡»å’Œæ‚¬å¿µçš„è¡¨ç°æ–¹å¼",
            "  4) ç”¨ç¬¦åˆé£æ ¼çš„è¯­è¨€å°†åˆ›æ„å…·è±¡åŒ–",
            "âœ¦ è¾“å‡ºå½¢å¼ï¼šä¸¤è¡Œå®Œæ•´çš„åç»­æ–‡æœ¬ï¼Œæ¯è¡Œç‹¬ç«‹æˆå¥",
            "âœ¦ é¿å…é™·é˜±ï¼š",
            "  Ã— å¹³åº¸å»¶ç»­ï¼šæœºæ¢°æ€§åœ°æ²¿ç€æœ€æ˜¾è€Œæ˜“è§çš„è·¯çº¿å‘å±•",
            "  Ã— ç”Ÿç¡¬è½¬æŠ˜ï¼šä¸ºäº†æ„å¤–è€Œæ„å¤–ï¼Œç¼ºä¹é“ºå«å’Œé€»è¾‘",
            "  Ã— é£æ ¼æ–­è£‚ï¼šè¿‡åº¦è¿½æ±‚åˆ›æ„è€Œåç¦»ä½œå“æ•´ä½“åŸºè°ƒ",
            "  Ã— ä¿¡æ¯è¿‡è½½ï¼šåœ¨ä¸¤è¡Œå†…å¡å…¥è¿‡å¤šæ–°å…ƒç´ å¯¼è‡´æ··ä¹±",
            ""
        ])
        
        # ===== ç¬¬å…­éƒ¨åˆ†ï¼šåˆ›æ„å¯å‘ç¤ºä¾‹ =====
        prompt_parts.extend([
            "ã€åˆ›æ„å¯å‘ç¤ºä¾‹ã€‘",
            "è‹¥åŸæ–‡æ˜¯è§’è‰²Aå‘Bå‘Šç™½ â†’",
            "  â€¢ ä¼ ç»Ÿç»­å†™ï¼šBå›åº”æ¥å—/æ‹’ç»",
            "  â€¢ åˆ›æ„æ€è·¯ï¼šCçªç„¶å‡ºç°æ‰“æ–­/Aè‡ªå·±è¯´åˆ°ä¸€åŠçªç„¶åœä¸‹/Bçš„ååº”å®Œå…¨å‡ºä¹Aæ„æ–™",
            "",
            "è‹¥åŸæ–‡æ˜¯è§’è‰²å‡†å¤‡è¡ŒåŠ¨ â†’",
            "  â€¢ ä¼ ç»Ÿç»­å†™ï¼šæŒ‰è®¡åˆ’æ‰§è¡Œ",
            "  â€¢ åˆ›æ„æ€è·¯ï¼šå‘ç°è®¡åˆ’è‡´å‘½æ¼æ´/å†…å¿ƒçªç„¶åŠ¨æ‘‡/æ„å¤–å‘ç°æ”¹å˜ä¸€åˆ‡çš„ä¿¡æ¯",
            ""
        ])
        
        prompt_parts.append("ç°åœ¨ï¼Œè¯·åŸºäºä¸Šè¿°åŸåˆ™ï¼Œä¸ºåˆ›ä½œè€…ç”Ÿæˆä»¤äººçœ¼å‰ä¸€äº®çš„åç»­ä¸¤è¡Œå‰§æƒ…ã€‚")
        
        return "\n".join(prompt_parts)
    
    def _get_default_features(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤ç‰¹å¾ï¼ˆå½“çŸ¥è¯†åº“ä¸ºç©ºæ—¶ä½¿ç”¨ï¼‰"""
        return {
            'avg_sentence_length': 20,
            'sentence_length_variance': 0,
            'punctuation_freq': {},
            'vocabulary_richness': 0.5,
            'common_phrases': [],
            'writing_style': {
                'has_dialogue': False,
                'descriptive_level': 'medium',
                'emotion_tone': 'neutral',
                'pacing': 'medium',
                'narrative_perspective': 'third',
                'dialogue_ratio': 0,
                'scene_change_frequency': 'medium'
            },
            'common_patterns': []
        }
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        if not text:
            return []
        
        # æŒ‰ç…§ä¸­æ–‡å¥å­ç»“æŸæ ‡è®°åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
        
        # è¿‡æ»¤ç©ºå¥å­å’Œå¤ªçŸ­çš„å¥å­
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 3]
        
        return sentences
    
    def _calculate_variance(self, numbers: List[float]) -> float:
        """è®¡ç®—æ–¹å·®"""
        if not numbers:
            return 0
        
        mean = sum(numbers) / len(numbers)
        variance = sum((x - mean) ** 2 for x in numbers) / len(numbers)
        return variance
    
    def _analyze_punctuation(self, text: str) -> Dict[str, float]:
        """åˆ†ææ ‡ç‚¹ç¬¦å·ä½¿ç”¨é¢‘ç‡"""
        if not text:
            return {}
        
        total_chars = len(text)
        if total_chars == 0:
            return {}
        
        return {
            'comma': text.count('ï¼Œ') / total_chars,
            'period': text.count('ã€‚') / total_chars,
            'exclamation': text.count('ï¼') / total_chars,
            'question': text.count('ï¼Ÿ') / total_chars,
            'ellipsis': text.count('â€¦') / total_chars,
            'colon': text.count('ï¼š') / total_chars,
            'semicolon': text.count('ï¼›') / total_chars,
        }
    
    def _analyze_vocabulary(self, text: str) -> float:
        """åˆ†æè¯æ±‡ä¸°å¯Œåº¦ï¼ˆä¸åŒè¯æ±‡æ•°/æ€»è¯æ±‡æ•°ï¼‰"""
        if not text:
            return 0.5
        
        # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæŒ‰æ ‡ç‚¹å’Œç©ºæ ¼åˆ†å‰²ï¼‰
        words = re.findall(r'[\u4e00-\u9fff]+', text)
        
        if not words:
            return 0.5
        
        unique_words = len(set(words))
        total_words = len(words)
        
        return unique_words / total_words if total_words > 0 else 0.5
    
    def _extract_common_phrases(self, texts: List[str], top_n: int = 10) -> List[str]:
        """æå–å¸¸è§çŸ­è¯­ï¼ˆ2-4å­—ï¼‰"""
        if not texts:
            return []
        
        # æå–æ‰€æœ‰2-4å­—çš„çŸ­è¯­
        phrase_counter = Counter()
        
        for text in texts:
            # ç§»é™¤æ ‡ç‚¹ç¬¦å·
            clean_text = re.sub(r'[^\u4e00-\u9fff]', '', text)
            
            # æå–2-4å­—çŸ­è¯­
            for length in [2, 3, 4]:
                for i in range(len(clean_text) - length + 1):
                    phrase = clean_text[i:i+length]
                    if len(phrase) == length:
                        phrase_counter[phrase] += 1
        
        # è¿‡æ»¤å‡ºç°æ¬¡æ•°å¤ªå°‘çš„çŸ­è¯­
        min_count = 2
        common_phrases = [phrase for phrase, count in phrase_counter.most_common(top_n * 2) if count >= min_count]
        
        return common_phrases[:top_n]
    
    def _analyze_writing_style(self, text: str, features: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå†™ä½œé£æ ¼"""
        style = {}
        
        # 1. æ˜¯å¦åŒ…å«å¯¹è¯
        style['has_dialogue'] = 'ã€Œ' in text or 'ã€' in text or '"' in text or '"' in text
        
        # 2. å¯¹è¯å æ¯”
        dialogue_chars = text.count('ã€Œ') + text.count('ã€') + text.count('"')
        total_chars = len(text)
        style['dialogue_ratio'] = dialogue_chars / total_chars if total_chars > 0 else 0
        
        # 3. æå†™ç¨‹åº¦ï¼ˆåŸºäºå½¢å®¹è¯å’Œå‰¯è¯çš„ä½¿ç”¨ï¼‰
        descriptive_words = ['çš„', 'åœ°', 'å¾—', 'å¾ˆ', 'ååˆ†', 'éå¸¸', 'æå…¶']
        descriptive_count = sum(text.count(word) for word in descriptive_words)
        descriptive_ratio = descriptive_count / total_chars if total_chars > 0 else 0
        
        if descriptive_ratio > 0.05:
            style['descriptive_level'] = 'high'
        elif descriptive_ratio < 0.02:
            style['descriptive_level'] = 'low'
        else:
            style['descriptive_level'] = 'medium'
        
        # 4. æƒ…æ„ŸåŸºè°ƒï¼ˆç®€å•å…³é”®è¯æ£€æµ‹ï¼‰
        positive_words = ['ç¬‘', 'é«˜å…´', 'å¿«ä¹', 'å¼€å¿ƒ', 'å–œæ‚¦', 'æ¸©æš–', 'ç¾å¥½']
        negative_words = ['å“­', 'æ‚²ä¼¤', 'éš¾è¿‡', 'ç—›è‹¦', 'å¤±æœ›', 'å†·']
        
        positive_count = sum(text.count(word) for word in positive_words)
        negative_count = sum(text.count(word) for word in negative_words)
        
        if positive_count > negative_count * 1.5:
            style['emotion_tone'] = 'positive'
        elif negative_count > positive_count * 1.5:
            style['emotion_tone'] = 'melancholic'
        else:
            style['emotion_tone'] = 'neutral'
        
        # 5. èŠ‚å¥å¿«æ…¢ï¼ˆåŸºäºå¥å­é•¿åº¦å’ŒåŠ¨è¯å¯†åº¦ï¼‰
        avg_length = features.get('avg_sentence_length', 20)
        action_words = ['èµ°', 'è·‘', 'è¯´', 'çœ‹', 'æ‹¿', 'æ‰“', 'æ¨', 'æ‹‰']
        action_count = sum(text.count(word) for word in action_words)
        action_ratio = action_count / total_chars if total_chars > 0 else 0
        
        if avg_length < 15 and action_ratio > 0.01:
            style['pacing'] = 'fast'
        elif avg_length > 30 or action_ratio < 0.005:
            style['pacing'] = 'slow'
        else:
            style['pacing'] = 'medium'
        
        # 6. å™äº‹è§†è§’ï¼ˆç®€å•æ£€æµ‹ï¼‰
        first_person_words = ['æˆ‘', 'å’±', 'ä¿º']
        first_person_count = sum(text.count(word) for word in first_person_words)
        
        if first_person_count / total_chars > 0.01:
            style['narrative_perspective'] = 'first'
        else:
            style['narrative_perspective'] = 'third'
        
        # 7. åœºæ™¯è½¬æ¢é¢‘ç‡ï¼ˆåŸºäºæ®µè½æ•°å’Œæ—¶é—´/åœ°ç‚¹è¯æ±‡ï¼‰
        paragraphs = text.split('\n\n')
        scene_markers = ['æ­¤æ—¶', 'è¿™æ—¶', 'çªç„¶', 'åæ¥', 'æ¥ç€', 'ç„¶å', 'äºæ˜¯']
        scene_marker_count = sum(text.count(word) for word in scene_markers)
        
        if len(paragraphs) > 10 and scene_marker_count > 5:
            style['scene_change_frequency'] = 'high'
        elif len(paragraphs) < 3 and scene_marker_count < 2:
            style['scene_change_frequency'] = 'low'
        else:
            style['scene_change_frequency'] = 'medium'
        
        return style
    
    def _extract_sentence_patterns(self, sentences: List[str], top_n: int = 5) -> List[str]:
        """æå–å¸¸è§å¥å¼æ¨¡å¼"""
        if not sentences:
            return []
        
        patterns = []
        
        # æå–ä¸€äº›å¸¸è§çš„å¥å¼ç‰¹å¾
        for sentence in sentences[:50]:  # åªåˆ†æå‰50ä¸ªå¥å­
            # 1. ç–‘é—®å¥
            if 'ï¼Ÿ' in sentence or 'å—' in sentence[-2:] or 'å‘¢' in sentence[-2:]:
                patterns.append('ç–‘é—®å¥å¼')
            
            # 2. æ„Ÿå¹å¥
            if 'ï¼' in sentence:
                patterns.append('æ„Ÿå¹å¥å¼')
            
            # 3. è½¬æŠ˜å¥
            if any(word in sentence for word in ['ä½†æ˜¯', 'ç„¶è€Œ', 'å¯æ˜¯', 'ä¸è¿‡']):
                patterns.append('è½¬æŠ˜å¥å¼')
            
            # 4. å› æœå¥
            if any(word in sentence for word in ['å› ä¸º', 'æ‰€ä»¥', 'å› æ­¤', 'ç”±äº']):
                patterns.append('å› æœå¥å¼')
            
            # 5. é€’è¿›å¥
            if any(word in sentence for word in ['è€Œä¸”', 'å¹¶ä¸”', 'ä¸ä»…', 'ç”šè‡³']):
                patterns.append('é€’è¿›å¥å¼')
        
        # ç»Ÿè®¡å¹¶è¿”å›æœ€å¸¸è§çš„æ¨¡å¼
        pattern_counter = Counter(patterns)
        return [pattern for pattern, _ in pattern_counter.most_common(top_n)]

