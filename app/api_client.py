from __future__ import annotations

import os
import sys
from typing import Any, Dict, Optional, List, TYPE_CHECKING
import time

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from app.config_manager import ConfigManager, APIConfig

# é¿å…å¾ªç¯å¯¼å…¥
if TYPE_CHECKING:
    from app.knowledge_base import KnowledgeBase, KnowledgeBaseManager, RerankClient


class AIError(RuntimeError):
    """è¡¨ç¤ºä¸ AI API äº¤äº’æ—¶å‘ç”Ÿçš„é”™è¯¯ã€‚"""


def truncate_context(text: str, max_chars: int = 1000) -> str:
    """æˆªå–ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œæœ€å¤šä¿ç•™ max_chars ä¸ªå­—ç¬¦ï¼Œä¿æŒå¥å­å®Œæ•´æ€§
    
    Args:
        text: å®Œæ•´æ–‡æœ¬
        max_chars: æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤1000å­—ï¼‰
        
    Returns:
        æˆªå–åçš„æ–‡æœ¬ï¼Œå¦‚æœåŸæ–‡ä¸è¶³ max_chars åˆ™è¿”å›å…¨éƒ¨
    """
    if not text:
        return ""
    
    # å¦‚æœæ–‡æœ¬é•¿åº¦ä¸è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥è¿”å›å…¨éƒ¨
    if len(text) <= max_chars:
        return text
    
    # ä»åå¾€å‰æˆªå– max_chars ä¸ªå­—ç¬¦
    truncated = text[-max_chars:]
    
    # å®šä¹‰å¥å­ç»“æŸæ ‡è®°ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰
    sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦', '.', '!', '?', '\n']
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´å¥å­çš„å¼€å§‹ä½ç½®ï¼ˆä»å‰å¾€åæ‰¾ç¬¬ä¸€ä¸ªå¥å­ç»“æŸæ ‡è®°ï¼‰
    first_sentence_end = -1
    for i, char in enumerate(truncated):
        if char in sentence_endings:
            first_sentence_end = i
            break
    
    # å¦‚æœæ‰¾åˆ°å¥å­ç»“æŸæ ‡è®°ï¼Œä»ä¸‹ä¸€ä¸ªå­—ç¬¦å¼€å§‹ï¼ˆä¿æŒå¥å­å®Œæ•´ï¼‰
    if first_sentence_end >= 0:
        # è·³è¿‡å¥å­ç»“æŸæ ‡è®°å’Œåç»­çš„ç©ºç™½å­—ç¬¦
        start_pos = first_sentence_end + 1
        while start_pos < len(truncated) and truncated[start_pos] in [' ', '\n', '\t', '\r']:
            start_pos += 1
        
        if start_pos < len(truncated):
            return truncated[start_pos:]
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥å­ç»“æŸæ ‡è®°ï¼Œè¿”å›åŸæˆªå–å†…å®¹
    return truncated


class AIClient:
    """AI æ¶¦è‰² API å®¢æˆ·ç«¯ã€‚"""

    _DEFAULT_BASE_URL = "https://api.siliconflow.cn/v1/chat/completions"

    def __init__(
        self,
        config_manager: Optional[ConfigManager] = None,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        base_url: Optional[str] = None,
        temperature: float = 0.2,
        timeout_seconds: int = 45,
    ) -> None:
        self._config_manager = config_manager
        
        # ä¼˜å…ˆä½¿ç”¨é…ç½®ç®¡ç†å™¨ä¸­çš„é…ç½®
        if self._config_manager:
            api_config = self._config_manager.get_api_config()
            self._api_key = api_key or api_config.api_key
            self._model = model or api_config.model
            self._base_url = base_url or api_config.base_url
        else:
            # å›é€€åˆ°ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼
            self._api_key = api_key or os.getenv("AI_API_KEY")
            self._model = model or os.getenv("AI_MODEL", "deepseek-ai/DeepSeek-V3.2-Exp")
            self._base_url = base_url or os.getenv("AI_BASE_URL", self._DEFAULT_BASE_URL)
        
        self._temperature = temperature
        self._timeout_seconds = timeout_seconds
        
        # ä¼˜åŒ–ï¼šé…ç½®è¿æ¥æ± å’Œé‡è¯•ç­–ç•¥ï¼ˆå¼±ç½‘ä¼˜åŒ–ï¼‰
        self._session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥ï¼šé’ˆå¯¹ç½‘ç»œé”™è¯¯å’Œä¸´æ—¶æ€§æ•…éšœè‡ªåŠ¨é‡è¯•
        retry_strategy = Retry(
            total=3,  # æœ€å¤šé‡è¯•3æ¬¡
            backoff_factor=0.5,  # é‡è¯•é—´éš”ï¼š0.5s, 1s, 2s
            status_forcelist=[408, 429, 500, 502, 503, 504],  # è¿™äº›çŠ¶æ€ç ä¼šé‡è¯•
            allowed_methods=["POST", "GET"],  # å…è®¸é‡è¯•çš„æ–¹æ³•
            raise_on_status=False  # ä¸åœ¨é‡è¯•åæŠ›å‡ºå¼‚å¸¸
        )
        
        # é…ç½®HTTPé€‚é…å™¨ï¼šä¼˜åŒ–è¿æ¥æ± 
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,  # è¿æ¥æ± å¤§å°
            pool_maxsize=20,  # æœ€å¤§è¿æ¥æ•°
            pool_block=False  # éé˜»å¡æ¨¡å¼
        )
        
        # ä¸ºhttpå’Œhttpséƒ½é…ç½®é€‚é…å™¨
        self._session.mount("http://", adapter)
        self._session.mount("https://", adapter)
        
        # å¯ç”¨keep-aliveå’Œå‹ç¼©
        self._session.headers.update({
            'Connection': 'keep-alive',
            'Accept-Encoding': 'gzip, deflate'
        })

    def _build_headers(self) -> Dict[str, str]:
        if not self._api_key:
            raise AIError("æœªé…ç½® AI API å¯†é’¥ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ AI_API_KEYã€‚")
        return {
            "Authorization": f"Bearer {self._api_key}",
            "Content-Type": "application/json",
        }
    
    def _make_request_with_retry(self, payload: Dict[str, Any], timeout: Optional[int] = None) -> Dict[str, Any]:
        """å‘é€è¯·æ±‚å¹¶å¤„ç†é‡è¯•é€»è¾‘ï¼ˆé’ˆå¯¹å¼±ç½‘ç¯å¢ƒä¼˜åŒ–ï¼‰
        
        Args:
            payload: è¯·æ±‚è´Ÿè½½
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            
        Returns:
            APIå“åº”çš„JSONæ•°æ®
            
        Raises:
            AIError: è¯·æ±‚å¤±è´¥æ—¶æŠ›å‡º
        """
        timeout = timeout or self._timeout_seconds
        last_error = None
        
        # æ‰‹åŠ¨å®ç°é¢å¤–çš„é‡è¯•é€»è¾‘ï¼ˆåœ¨Sessioné‡è¯•ä¹‹å¤–ï¼‰
        # è¿™æ ·å¯ä»¥æ›´å¥½åœ°å¤„ç†è¶…æ—¶å’Œè¿æ¥é”™è¯¯
        for attempt in range(2):  # é¢å¤–é‡è¯•1æ¬¡
            try:
                response = self._session.post(
                    self._base_url,
                    headers=self._build_headers(),
                    json=payload,
                    timeout=timeout,
                )
                
                # å¤„ç†HTTPé”™è¯¯
                if response.status_code >= 500:
                    last_error = AIError("AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚")
                    if attempt < 1:  # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼š
                        time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                        continue
                    raise last_error
                
                if response.status_code == 401:
                    raise AIError("AI API è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¯†é’¥æ˜¯å¦æ­£ç¡®ã€‚")
                
                if not response.ok:
                    raise AIError(f"è¯·æ±‚å¤±è´¥ï¼š{response.status_code} {response.text}")
                
                # è§£æå“åº”
                try:
                    data: Dict[str, Any] = response.json()
                    return data
                except ValueError as exc:
                    raise AIError("æ— æ³•è§£æ AI å“åº”ï¼Œè¯·ç¨åå†è¯•ã€‚") from exc
                    
            except requests.Timeout as exc:
                last_error = AIError(f"è¯·æ±‚è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚")
                if attempt < 1:  # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼š
                    time.sleep(1)
                    continue
                raise last_error from exc
                
            except requests.ConnectionError as exc:
                last_error = AIError("ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®ã€‚")
                if attempt < 1:
                    time.sleep(1)
                    continue
                raise last_error from exc
                
            except requests.RequestException as exc:
                last_error = AIError("ç½‘ç»œå¼‚å¸¸ï¼Œæ— æ³•è¿æ¥è‡³ AI æœåŠ¡ã€‚")
                if attempt < 1:
                    time.sleep(1)
                    continue
                raise last_error from exc
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        if last_error:
            raise last_error
        raise AIError("è¯·æ±‚å¤±è´¥")
    
    def _print_token_usage(self, usage_data: Dict[str, Any], operation: str = "APIè°ƒç”¨"):
        """æ‰“å°Tokenä½¿ç”¨ç»Ÿè®¡
        
        Args:
            usage_data: APIè¿”å›çš„usageæ•°æ®
            operation: æ“ä½œæè¿°
        """
        if not usage_data:
            return
        
        input_tokens = usage_data.get('input_tokens', usage_data.get('prompt_tokens', 0))
        output_tokens = usage_data.get('output_tokens', usage_data.get('completion_tokens', 0))
        total_tokens = usage_data.get('total_tokens', input_tokens + output_tokens)
        
        # DeepSeek-V3.2-Exp ä»·æ ¼å‚è€ƒ
        # DeepSeek-V3.2-Exp: è¾“å…¥ 2å…ƒ/ç™¾ä¸‡tokens, è¾“å‡º 3å…ƒ/ç™¾ä¸‡tokens
        # å®˜ç½‘ï¼šhttps://platform.deepseek.com/api-docs/pricing/
        
        # æ ¹æ®æ¨¡å‹ä¼°ç®—æˆæœ¬ï¼ˆä½¿ç”¨DeepSeek-V3.2-Expä»·æ ¼ï¼‰
        input_cost = (input_tokens / 1_000_000) * 2.0
        output_cost = (output_tokens / 1_000_000) * 3.0
        total_cost = input_cost + output_cost
        
        print("=" * 60)
        print(f"ğŸ“Š ã€Tokenæ¶ˆè€—ç»Ÿè®¡ - {operation}ã€‘")
        print(f"   è¾“å…¥tokens: {input_tokens:,}")
        print(f"   è¾“å‡ºtokens: {output_tokens:,}")
        print(f"   æ€»è®¡tokens: {total_tokens:,}")
        print(f"   é¢„ä¼°æˆæœ¬: Â¥{total_cost:.4f} (æŒ‰DeepSeek-V3.2-Expä»·æ ¼)")
        print(f"   è¾“å…¥æˆæœ¬: Â¥{input_cost:.6f} (Â¥2/ç™¾ä¸‡)")
        print(f"   è¾“å‡ºæˆæœ¬: Â¥{output_cost:.6f} (Â¥3/ç™¾ä¸‡)")
        print("=" * 60)

    def polish_text(self, text: str) -> str:
        payload = {
            "model": self._model,
            "messages": [
                {
                    "role": "system",
                    "content": (
                        "ä½ æ˜¯ä¸€ä½èµ„æ·±ä¸­æ–‡å°è¯´ç¼–è¾‘ï¼Œè¯·åœ¨ä¿ç•™åŸæ„çš„å‰æä¸‹æ¶¦è‰²ç”¨æˆ·æ–‡æœ¬ã€‚"
                        "ä¼˜åŒ–ç”¨è¯ã€å¥å¼ä¸èŠ‚å¥ï¼Œè¾“å‡ºæ¶¦è‰²åçš„å®Œæ•´æ–‡æœ¬ï¼Œä¸è¦åŒ…å«å¤šä½™è§£é‡Šã€‚"
                    ),
                },
                {"role": "user", "content": text},
            ],
            "temperature": self._temperature,
            "stream": False,
        }

        # ä½¿ç”¨ä¼˜åŒ–çš„è¯·æ±‚æ–¹æ³•ï¼ˆåŒ…å«é‡è¯•å’Œé”™è¯¯å¤„ç†ï¼‰
        data = self._make_request_with_retry(payload)

        choices = data.get("choices")
        if not choices:
            raise AIError("AI æœªè¿”å›å†…å®¹ï¼Œè¯·ç¨åå†è¯•ã€‚")

        message = choices[0].get("message", {})
        content = message.get("content")
        if not content:
            raise AIError("AI å“åº”å†…å®¹ä¸ºç©ºã€‚")
        
        # æ‰“å°Tokenä½¿ç”¨ç»Ÿè®¡
        if "usage" in data:
            self._print_token_usage(data["usage"], operation="æ¶¦è‰²")

        return content.strip()

    # æ–°å¢ï¼šä»…æ¶¦è‰²æœ€åä¸€è¡Œï¼Œå‘é€æœ€åäº”è¡Œä¸Šä¸‹æ–‡
    def polish_last_line(self, context_lines: List[str], target_line: str, style_prompt: str = "") -> str:
        import sys
        print(f"[DEBUG API] polish_last_line å¼€å§‹ï¼Œtarget_line={target_line[:30]}, contextè¡Œæ•°={len(context_lines)}", flush=True)
        sys.stdout.flush()
        
        context_text = "\n".join(context_lines) if context_lines else "(æ— )"
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯ - æ¶¦è‰²ä¸“æ³¨äºä¼˜åŒ–è¡¨è¾¾ï¼Œä¸åšé¢„æµ‹æ€§åˆ›ä½œ
        system_content = "ä½ æ˜¯ä¸€ä½èµ„æ·±ä¸­æ–‡å°è¯´ç¼–è¾‘ã€‚"
        
        # å¦‚æœæœ‰é£æ ¼è¦æ±‚ï¼Œå°†å…¶ä½œä¸ºäººè®¾çš„ä¸€éƒ¨åˆ†
        if style_prompt:
            system_content += f"\n\nã€ä½ çš„æ¶¦è‰²é£æ ¼ã€‘\n{style_prompt}"
        
        # æ·»åŠ æ ¸å¿ƒä»»åŠ¡æŒ‡ä»¤ - çº¯ç²¹çš„æ¶¦è‰²ï¼Œä¸åšåˆ›é€ æ€§æ”¹å†™
        system_content += (
            "\n\nã€æ ¸å¿ƒä»»åŠ¡ã€‘\n"
            "å¯¹æœ€åä¸€è¡Œæ–‡æœ¬è¿›è¡Œæ¶¦è‰²ä¼˜åŒ–ï¼Œä¿æŒåŸæ„å’Œæƒ…èŠ‚ï¼Œæå‡è¡¨è¾¾è´¨é‡ã€‚\n"
            "\n"
            "ã€æ¶¦è‰²è¦æ±‚ã€‘\n"
            "1. ä¿æŒåŸæ–‡çš„æ ¸å¿ƒæ„æ€ã€æƒ…èŠ‚å’Œäººç‰©åŠ¨ä½œä¸å˜\n"
            "2. ä¼˜åŒ–ç”¨è¯ã€å¥å¼ã€èŠ‚å¥ï¼Œæå‡æ–‡å­—çš„æµç•…åº¦å’Œå¯è¯»æ€§\n"
            "3. ä¿®æ­£è¯­æ³•é—®é¢˜å’Œä¸å½“è¡¨è¾¾ï¼Œè®©æ–‡å­—æ›´é€šé¡ºè‡ªç„¶\n"
            "4. ç¬¦åˆä¸Šè¿°é£æ ¼è¦æ±‚ï¼Œä¿æŒæ–‡æœ¬çš„æ•´ä½“é£æ ¼ç»Ÿä¸€\n"
            "5. ä»…åšæ¶¦è‰²ä¼˜åŒ–ï¼Œä¸è¦æ·»åŠ æ–°æƒ…èŠ‚æˆ–æ”¹å˜åŸæ„\n"
            "\n"
            "ã€è¾“å‡ºè¦æ±‚ã€‘\n"
            "1. åªè¾“å‡ºæ¶¦è‰²åçš„é‚£ä¸€è¡Œæ–‡æœ¬ï¼Œä¸è¦è¾“å‡ºä¸Šä¸‹æ–‡\n"
            "2. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–æ ‡æ³¨\n"
            "3. ç›´æ¥è¾“å‡ºæ¶¦è‰²åçš„æ–‡æœ¬å†…å®¹å³å¯\n"
        )
        
        payload = {
            "model": self._model,
            "messages": [
                {
                    "role": "system",
                    "content": system_content,
                },
                {
                    "role": "user",
                    "content": f"ä¸Šä¸‹æ–‡ï¼š\n{context_text}\n\nå¾…æ¶¦è‰²æ–‡æœ¬ï¼š\n{target_line}\n\nè¯·è¾“å‡ºæ¶¦è‰²åçš„æ–‡æœ¬ï¼š",
                },
            ],
            "temperature": self._temperature,
            "stream": False,
        }

        print(f"[DEBUG API] å‡†å¤‡å‘é€è¯·æ±‚åˆ° {self._base_url}, model={self._model}", flush=True)
        sys.stdout.flush()
        
        # ä½¿ç”¨ä¼˜åŒ–çš„è¯·æ±‚æ–¹æ³•ï¼ˆåŒ…å«é‡è¯•å’Œé”™è¯¯å¤„ç†ï¼‰
        data = self._make_request_with_retry(payload)
        
        print(f"[DEBUG API] è§£æJSONæˆåŠŸ", flush=True)
        sys.stdout.flush()

        choices = data.get("choices")
        if not choices:
            print(f"[DEBUG API] æ²¡æœ‰choices", flush=True)
            sys.stdout.flush()
            raise AIError("AI æœªè¿”å›å†…å®¹ï¼Œè¯·ç¨åå†è¯•ã€‚")

        message = choices[0].get("message", {})
        content = message.get("content")
        if not content:
            print(f"[DEBUG API] contentä¸ºç©º", flush=True)
            sys.stdout.flush()
            raise AIError("AI å“åº”å†…å®¹ä¸ºç©ºã€‚")

        print(f"[DEBUG API] polish_last_line å®Œæˆï¼Œè¿”å›å†…å®¹é•¿åº¦: {len(content)}", flush=True)
        print(f"[DEBUG API] contentåŸå§‹å†…å®¹: {repr(content)}", flush=True)
        sys.stdout.flush()
        
        # æ‰“å°Tokenä½¿ç”¨ç»Ÿè®¡
        if "usage" in data:
            self._print_token_usage(data["usage"], operation="æ¶¦è‰²æœ€åä¸€è¡Œ")
        
        result = content.strip()
        print(f"[DEBUG API] stripåå†…å®¹: {repr(result)}", flush=True)
        print(f"[DEBUG API] å‡†å¤‡è¿”å›ï¼Œé•¿åº¦: {len(result)}", flush=True)
        sys.stdout.flush()
        
        return result

    def optimize_prompt(self, prompt_text: str) -> str:
        if not prompt_text or not prompt_text.strip():
            raise AIError("æç¤ºè¯ä¸ºç©ºï¼Œæ— æ³•ä¼˜åŒ–ã€‚")
        payload = {
            "model": self._model,
            "messages": [
                {
                    "role": "system",
                    "content": (
                        "ä½ æ˜¯ä¸€ä½èµ„æ·±ä¸­æ–‡æç¤ºè¯å·¥ç¨‹å¸ˆã€‚" \
                        "è¯·å°†ç”¨æˆ·æä¾›çš„æ¶¦è‰²é£æ ¼æç¤ºè¯è¿›è¡Œç»“æ„åŒ–ä¸å¢å¼ºï¼š" \
                        "æ˜ç¡®ç›®æ ‡ã€é£æ ¼è¦ç‚¹ã€çº¦æŸã€è¾“å‡ºè¦æ±‚ï¼›é¿å…å†—ä½™ä¸å«ç³Šï¼›" \
                        "ä¿æŒä¸­æ–‡è¾“å‡ºï¼Œåªè¿”å›ä¼˜åŒ–åçš„æç¤ºè¯æœ¬èº«ï¼Œä¸è¦è§£é‡Šã€‚"
                    ),
                },
                {"role": "user", "content": prompt_text},
            ],
            "temperature": self._temperature,
            "stream": False,
        }
        
        # ä½¿ç”¨ä¼˜åŒ–çš„è¯·æ±‚æ–¹æ³•ï¼ˆåŒ…å«é‡è¯•å’Œé”™è¯¯å¤„ç†ï¼‰
        data = self._make_request_with_retry(payload)
        
        # æ‰“å°Tokenä½¿ç”¨ç»Ÿè®¡
        if "usage" in data:
            self._print_token_usage(data["usage"], operation="ä¼˜åŒ–æç¤ºè¯")
        
        choices = data.get("choices")
        if not choices:
            raise AIError("AI æœªè¿”å›å†…å®¹ï¼Œè¯·ç¨åå†è¯•ã€‚")
        message = choices[0].get("message", {})
        content = message.get("content")
        if not content:
            raise AIError("AI å“åº”å†…å®¹ä¸ºç©ºã€‚")
        return content.strip()

    def update_config(self, config_manager: ConfigManager) -> None:
        """æ›´æ–°é…ç½®ç®¡ç†å™¨"""
        self._config_manager = config_manager
        if self._config_manager:
            api_config = self._config_manager.get_api_config()
            self._api_key = api_config.api_key
            self._model = api_config.model
            self._base_url = api_config.base_url
    
    def predict_plot_continuation(self, full_text: str, style_prompt: str = "") -> str:
        """é¢„æµ‹å‰§æƒ…å‘å±•ï¼Œç”Ÿæˆæ¥ä¸‹æ¥ä¸¤è¡Œå†…å®¹
        
        Args:
            full_text: å½“å‰ç¼–è¾‘å™¨ä¸­çš„å…¨éƒ¨æ–‡æœ¬å†…å®¹
            style_prompt: é£æ ¼æç¤ºè¯ï¼ˆå¯é€‰ï¼‰ï¼Œå°†ä½œä¸ºäººè®¾å‘é€ç»™AI
            
        Returns:
            é¢„æµ‹çš„æ¥ä¸‹æ¥ä¸¤è¡Œå‰§æƒ…å†…å®¹
        """
        if not full_text or not full_text.strip():
            raise AIError("æ–‡æœ¬å†…å®¹ä¸ºç©ºï¼Œæ— æ³•é¢„æµ‹å‰§æƒ…ã€‚")
        
        # æˆªå–ä¸Šä¸‹æ–‡ï¼šæœ€å¤š1000å­—ï¼Œä¿æŒå¥å­å®Œæ•´
        context_text = truncate_context(full_text, max_chars=1000)
        
        # æ„å»ºé¢„æµ‹ç³»ç»Ÿæç¤ºè¯ - æ˜ç¡®æ ‡æ³¨ä¸º"å‰§æƒ…é¢„æµ‹"ä»»åŠ¡
        system_content = (
            "ä½ æ˜¯ä¸€ä½å¯Œæœ‰åˆ›é€ åŠ›çš„èµ„æ·±ä¸­æ–‡å°è¯´ä½œå®¶ã€‚\n"
            "\n"
            "ã€ä½ çš„ä»»åŠ¡ã€‘å‰§æƒ…é¢„æµ‹ä¸åˆ›ä½œ\n"
            "ä¸ºåˆ›ä½œè€…æä¾›æ–°é¢–çš„å‰§æƒ…æ€è·¯ï¼Œç”Ÿæˆæ¥ä¸‹æ¥çš„æ•…äº‹å†…å®¹ã€‚\n"
            "è¿™ä¸æ˜¯æ¶¦è‰²ä»»åŠ¡ï¼Œè€Œæ˜¯åˆ›é€ æ€§çš„ç»­å†™å’Œå‰§æƒ…å‘å±•ã€‚\n"
            "\n"
            "ã€æ ¸å¿ƒåŸåˆ™ã€‘\n"
            "ğŸ”´ æœ€é‡è¦ï¼šå¿…é¡»ç´§å¯†åŸºäºç”¨æˆ·å½“å‰ä¹¦å†™çš„å‰§æƒ…å†…å®¹æ¥é¢„æµ‹\n"
            "ğŸ­ åˆ›æ„å‘å±•ï¼šåœ¨å½“å‰å‰§æƒ…åŸºç¡€ä¸Šï¼Œæä¾›æœ‰å¼ åŠ›çš„åç»­å‘å±•\n"
            "ğŸ”— æ— ç¼è¡”æ¥ï¼šé¢„æµ‹å†…å®¹å¿…é¡»ç›´æ¥æ¥ç»­å½“å‰æ–‡æœ¬æœ«å°¾ï¼Œä¸èƒ½è·³è·ƒ"
        )
        
        # å¦‚æœæœ‰é£æ ¼æç¤ºè¯ï¼Œå°†å…¶ä½œä¸ºå†™ä½œäººè®¾çš„ä¸€éƒ¨åˆ†
        if style_prompt:
            system_content += f"\n\nã€ä½ çš„å†™ä½œé£æ ¼ã€‘\n{style_prompt}"
        
        # æ·»åŠ é¢„æµ‹ä»»åŠ¡è¦æ±‚
        system_content += (
            "\n\nã€å‰§æƒ…é¢„æµ‹è¦æ±‚ï¼ˆæƒé‡æ’åºï¼‰ã€‘\n"
            "1. ğŸ”´ ç´§æ‰£å½“å‰å‰§æƒ…ï¼šå¿…é¡»åŸºäºã€å½“å‰å‰§æƒ…ã€‘çš„å…·ä½“æƒ…å¢ƒã€äººç‰©çŠ¶æ€ã€åœºæ™¯ç»†èŠ‚\n"
            "2. ğŸ­ åˆ›æ„å‘å±•ï¼šä»å½“å‰æƒ…å¢ƒå‡ºå‘ï¼Œé€‰æ‹©æœ€æœ‰æˆå‰§å¼ åŠ›å’Œæƒ…æ„Ÿå†²å‡»çš„å‘å±•æ–¹å‘\n"
            "3. âœ… é€»è¾‘åˆç†ï¼šç¡®ä¿é¢„æµ‹æ—¢æ–°é¢–åˆç¬¦åˆå·²å»ºç«‹çš„é€»è¾‘å’Œæƒ…å¢ƒ\n"
            "4. ğŸ¨ é£æ ¼å¥‘åˆï¼šç”¨ç¬¦åˆä¸Šè¿°å†™ä½œé£æ ¼çš„è¯­è¨€è¡¨è¾¾\n"
            "5. ğŸ”— æ— ç¼è¡”æ¥ï¼šè¾“å‡ºå¿…é¡»ç›´æ¥æ¥ç»­å½“å‰å‰§æƒ…çš„æœ«å°¾ï¼Œä¸èƒ½è·³è·ƒæˆ–è„±èŠ‚\n"
            "\n"
            "ã€è¾“å‡ºè¦æ±‚ã€‘\n"
            "- åªè¾“å‡ºä¸¤è¡Œçº¯æ–‡æœ¬ï¼ˆæ¯è¡Œä¸€ä¸ªå®Œæ•´å¥å­ï¼‰\n"
            "- ä¸è¦ä»»ä½•è§£é‡Šã€æ ‡æ³¨æˆ–å…ƒæ•°æ®\n"
            "- ç›´æ¥è¾“å‡ºé¢„æµ‹çš„åç»­å‰§æƒ…"
        )
        
        payload = {
            "model": self._model,
            "messages": [
                {
                    "role": "system",
                    "content": system_content,
                },
                {
                    "role": "user",
                    "content": f"ã€å½“å‰å‰§æƒ…ã€‘\n{context_text}\n\nè¯·åŸºäºä¸Šè¿°å‰§æƒ…ï¼Œç”Ÿæˆä»¤äººçœ¼å‰ä¸€äº®çš„åç»­ä¸¤è¡Œå†…å®¹ï¼ˆç›´æ¥è¾“å‡ºä¸¤è¡Œæ–‡æœ¬ï¼‰ï¼š"
                },
            ],
            "temperature": 0.85,  # ä½¿ç”¨è¾ƒé«˜æ¸©åº¦ä»¥å¢åŠ åˆ›é€ æ€§å’Œæ„å¤–æ€§
            "stream": False,
        }
        
        # ä½¿ç”¨ä¼˜åŒ–çš„è¯·æ±‚æ–¹æ³•ï¼ˆåŒ…å«é‡è¯•å’Œé”™è¯¯å¤„ç†ï¼‰
        data = self._make_request_with_retry(payload)
        
        # æ‰“å°Tokenä½¿ç”¨ç»Ÿè®¡
        if "usage" in data:
            self._print_token_usage(data["usage"], operation="å‰§æƒ…é¢„æµ‹")
        
        choices = data.get("choices")
        if not choices:
            raise AIError("AI æœªè¿”å›å†…å®¹ï¼Œè¯·ç¨åå†è¯•ã€‚")
        
        message = choices[0].get("message", {})
        content = message.get("content")
        if not content:
            raise AIError("AI å“åº”å†…å®¹ä¸ºç©ºã€‚")
        
        return content.strip()
    
    def polish_last_line_with_kb(
        self,
        context_lines: List[str],
        target_line: str,
        kb_manager: 'KnowledgeBaseManager',
        character_kb: Optional['KnowledgeBase'] = None,
        outline_kbs: Optional[List['KnowledgeBase']] = None,
        character_kbs: Optional[List['KnowledgeBase']] = None,
        rerank_client: Optional['RerankClient'] = None,
        style_prompt: str = "",
        min_relevance_threshold: float = 0.25
    ) -> str:
        """åŸºäºå¤§çº²å’Œäººè®¾çŸ¥è¯†åº“çš„å¢å¼ºæ¶¦è‰²
        
        æ³¨æ„ï¼šæ¶¦è‰²åŒæ—¶ä½¿ç”¨å¤§çº²çŸ¥è¯†åº“å’Œäººè®¾çŸ¥è¯†åº“ï¼Œæœ€å¤š5æ¡ä¸Šä¸‹æ–‡
        
        Args:
            context_lines: ä¸Šä¸‹æ–‡è¡Œ
            target_line: å¾…æ¶¦è‰²çš„ç›®æ ‡è¡Œ
            kb_manager: çŸ¥è¯†åº“ç®¡ç†å™¨
            character_kb: äººè®¾çŸ¥è¯†åº“ï¼ˆå¯é€‰ï¼Œå‘åå…¼å®¹ï¼‰
            outline_kbs: å¤§çº²çŸ¥è¯†åº“åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            character_kbs: äººè®¾çŸ¥è¯†åº“åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            rerank_client: é‡æ’åºå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
            style_prompt: é£æ ¼æç¤ºè¯
            min_relevance_threshold: æœ€å°ç›¸å…³æ€§é˜ˆå€¼
            
        Returns:
            æ¶¦è‰²åçš„æ–‡æœ¬
        """
        # å‘åå…¼å®¹ï¼šå¦‚æœä¼ å…¥äº† character_kbï¼Œå°†å…¶æ·»åŠ åˆ° character_kbs åˆ—è¡¨
        if character_kb and character_kb.documents:
            if character_kbs is None:
                character_kbs = [character_kb]
            elif character_kb not in character_kbs:
                character_kbs = list(character_kbs) + [character_kb]
        
        # 1. å¦‚æœæ—¢æ²¡æœ‰å¤§çº²çŸ¥è¯†åº“ä¹Ÿæ²¡æœ‰äººè®¾çŸ¥è¯†åº“ï¼Œå›é€€åˆ°æ™®é€šæ¶¦è‰²
        has_outline = outline_kbs and any(kb.documents for kb in outline_kbs)
        has_character = character_kbs and any(kb.documents for kb in character_kbs)
        
        if not has_outline and not has_character:
            print("[INFO] å¤§çº²å’Œäººè®¾çŸ¥è¯†åº“å‡ä¸ºç©ºï¼Œä½¿ç”¨æ™®é€šæ¶¦è‰²")
            return self.polish_last_line(context_lines, target_line, style_prompt)
        
        # 2. ä½¿ç”¨ä¸Šä¸‹æ–‡å’Œç›®æ ‡è¡Œæ„å»ºæŸ¥è¯¢
        query_context = "\n".join(context_lines[-2:]) if context_lines else ""  # æœ€åä¸¤è¡Œä¸Šä¸‹æ–‡
        query_text = f"{query_context}\n{target_line}"
        
        try:
            # 3. ä»å¤§çº²å’Œäººè®¾çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³å†…å®¹
            all_similar_docs = []
            
            # ä»å¤§çº²çŸ¥è¯†åº“æ£€ç´¢
            if outline_kbs:
                for outline_kb in outline_kbs:
                    if outline_kb.documents:
                        print(f"[INFO] å¼€å§‹å¤§çº²çŸ¥è¯†åº“æ£€ç´¢: {outline_kb.name}")
                        outline_docs = kb_manager.search_similar_documents(
                            query_text=query_text,
                            kb=outline_kb,
                            top_k=20,
                            rerank_client=rerank_client,
                            final_top_n=5
                        )
                        if outline_docs:
                            # æ ‡è®°æ¥æº
                            for doc_item in outline_docs:
                                doc_item['kb_source'] = 'outline'
                                doc_item['kb_name'] = outline_kb.name
                                doc_item['kb_obj'] = outline_kb
                            all_similar_docs.extend(outline_docs)
                            print(f"[INFO] å¤§çº²çŸ¥è¯†åº“ {outline_kb.name} æ£€ç´¢åˆ° {len(outline_docs)} ä¸ªæ–‡æ¡£")
            
            # ä»äººè®¾çŸ¥è¯†åº“æ£€ç´¢
            if character_kbs:
                for character_kb_item in character_kbs:
                    if character_kb_item.documents:
                        print(f"[INFO] å¼€å§‹äººè®¾çŸ¥è¯†åº“æ£€ç´¢: {character_kb_item.name}")
                        character_docs = kb_manager.search_similar_documents(
                            query_text=query_text,
                            kb=character_kb_item,
                            top_k=20,
                            rerank_client=rerank_client,
                            final_top_n=5
                        )
                        if character_docs:
                            # æ ‡è®°æ¥æº
                            for doc_item in character_docs:
                                doc_item['kb_source'] = 'character'
                                doc_item['kb_name'] = character_kb_item.name
                                doc_item['kb_obj'] = character_kb_item
                            all_similar_docs.extend(character_docs)
                            print(f"[INFO] äººè®¾çŸ¥è¯†åº“ {character_kb_item.name} æ£€ç´¢åˆ° {len(character_docs)} ä¸ªæ–‡æ¡£")
            
            # 4. åˆå¹¶å¹¶æŒ‰ç›¸å…³æ€§æ’åºæ‰€æœ‰æ£€ç´¢ç»“æœ
            if all_similar_docs:
                all_similar_docs.sort(
                    key=lambda x: x.get('relevance_score', x.get('similarity_score', 0)),
                    reverse=True
                )
                print(f"[INFO] åˆå¹¶åå…±æ£€ç´¢åˆ° {len(all_similar_docs)} ä¸ªæ–‡æ¡£")
            
            # 5. è¿‡æ»¤ä½è´¨é‡ç»“æœ
            filtered_docs = []
            if all_similar_docs:
                max_score = all_similar_docs[0].get('relevance_score', all_similar_docs[0].get('similarity_score', 0))
                if max_score >= 0.7:
                    dynamic_threshold = max(min_relevance_threshold, max_score * 0.4)
                elif max_score >= 0.4:
                    dynamic_threshold = max(min_relevance_threshold, max_score * 0.3)
                else:
                    dynamic_threshold = min_relevance_threshold
                
                for doc_item in all_similar_docs:
                    score = doc_item.get('relevance_score', doc_item.get('similarity_score', 0))
                    if score >= dynamic_threshold:
                        filtered_docs.append(doc_item)
                
                if not filtered_docs and all_similar_docs:
                    filtered_docs = all_similar_docs[:min(2, len(all_similar_docs))]
            
            print(f"[INFO] çŸ¥è¯†åº“æ£€ç´¢ï¼šæ‰¾åˆ° {len(all_similar_docs)} ä¸ªæ–‡æ¡£ï¼Œè¿‡æ»¤åä¿ç•™ {len(filtered_docs)} ä¸ª")
            
            # 6. å¦‚æœæ²¡æœ‰ç›¸å…³ç»“æœï¼Œå›é€€åˆ°æ™®é€šæ¶¦è‰²
            if not filtered_docs:
                print("[INFO] æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å¤§çº²/äººè®¾å†…å®¹ï¼Œä½¿ç”¨æ™®é€šæ¶¦è‰²")
                return self.polish_last_line(context_lines, target_line, style_prompt)
            
            # é™åˆ¶æœ€å¤š5æ¡ä¸Šä¸‹æ–‡
            filtered_docs = filtered_docs[:5]
            
            # 7. æå–çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼ˆåŒºåˆ†å¤§çº²å’Œäººè®¾ï¼‰
            outline_contexts = []
            character_contexts = []
            
            for doc_item in filtered_docs:
                doc = doc_item['document']
                kb_source = doc_item.get('kb_source', 'character')
                kb_obj = doc_item.get('kb_obj')
                
                if kb_obj:
                    doc_with_context = kb_manager.get_document_with_context(
                        doc=doc,
                        kb=kb_obj,
                        context_lines_before=2,
                        context_lines_after=2
                    )
                    
                    context_item = {
                        'content': doc.content,
                        'full_context': doc_with_context['full_context'],
                        'score': doc_item.get('relevance_score', doc_item.get('similarity_score', 0)),
                        'kb_name': doc_item.get('kb_name', '')
                    }
                    
                    if kb_source == 'outline':
                        outline_contexts.append(context_item)
                    else:
                        character_contexts.append(context_item)
            
            # 8. æ„å»ºå¢å¼ºçš„æ¶¦è‰²promptï¼ˆç»“æ„åŒ–æ ‡æ³¨ï¼‰
            system_content = "ä½ æ˜¯ä¸€ä½èµ„æ·±ä¸­æ–‡å°è¯´ç¼–è¾‘ã€‚"
            
            # æ·»åŠ é£æ ¼è¦æ±‚
            if style_prompt:
                system_content += f"\n\nã€ä½ çš„æ¶¦è‰²é£æ ¼ã€‘\n{style_prompt}"
            
            # æ·»åŠ ä»»åŠ¡è¯´æ˜ï¼ˆæ ¹æ®æœ‰å“ªäº›çŸ¥è¯†åº“åŠ¨æ€è°ƒæ•´ï¼‰
            task_description = "\n\n"
            
            if outline_contexts and character_contexts:
                # åŒæ—¶æœ‰å¤§çº²å’Œäººè®¾
                task_description += (
                    "ã€å¤§çº²å’Œäººè®¾èµ„æ–™çš„ä½œç”¨ã€‘\n"
                    "â€¢ å¤§çº²èµ„æ–™ï¼šæ­ç¤ºæ•…äº‹çš„æ•´ä½“æ¡†æ¶ã€å‰§æƒ…èµ°å‘ã€å…³é”®äº‹ä»¶ã€ä¸–ç•Œè§‚è®¾å®šç­‰å®è§‚ä¿¡æ¯\n"
                    "â€¢ äººè®¾èµ„æ–™ï¼šæ­ç¤ºäººç‰©çš„æ€§æ ¼ã€èƒŒæ™¯ã€è¡Œä¸ºæ¨¡å¼ã€è¯­è¨€ä¹ æƒ¯ç­‰æ ¸å¿ƒç‰¹å¾\n"
                    "\n"
                    "è¯·åŸºäºè¿™äº›èµ„æ–™å¯¹æ–‡æœ¬è¿›è¡Œæ¶¦è‰²ï¼Œç¡®ä¿ï¼š\n"
                    "1. å‰§æƒ…è¡¨è¾¾ç¬¦åˆå¤§çº²è®¾å®šï¼Œä¸æ•´ä½“æ•…äº‹æ¡†æ¶ä¿æŒä¸€è‡´\n"
                    "2. äººç‰©æ€§æ ¼å’Œè¡Œä¸ºçš„è¡¨è¾¾ç¬¦åˆäººè®¾å®šä½\n"
                    "3. å¯¹è¯å’Œå¿ƒç†æ´»åŠ¨çš„æªè¾ç¬¦åˆè§’è‰²ä¸ªæ€§å’Œè¯­è¨€ä¹ æƒ¯\n"
                    "4. ç»†èŠ‚æå†™çš„ç”¨è¯å¥‘åˆä¸–ç•Œè§‚å’Œäººç‰©èƒŒæ™¯\n"
                    "5. åœ¨ç¬¦åˆè®¾å®šçš„å‰æä¸‹ï¼Œä¼˜åŒ–è¡¨è¾¾è´¨é‡\n"
                )
            elif outline_contexts:
                # åªæœ‰å¤§çº²
                task_description += (
                    "ã€å¤§çº²èµ„æ–™çš„ä½œç”¨ã€‘\n"
                    "å¤§çº²èµ„æ–™æ­ç¤ºäº†æ•…äº‹çš„æ•´ä½“æ¡†æ¶ã€å‰§æƒ…èµ°å‘ã€å…³é”®äº‹ä»¶ã€ä¸–ç•Œè§‚è®¾å®šç­‰å®è§‚ä¿¡æ¯ã€‚\n"
                    "\n"
                    "è¯·åŸºäºå¤§çº²èµ„æ–™å¯¹æ–‡æœ¬è¿›è¡Œæ¶¦è‰²ï¼Œç¡®ä¿ï¼š\n"
                    "1. å‰§æƒ…è¡¨è¾¾ç¬¦åˆå¤§çº²è®¾å®šï¼Œä¸æ•´ä½“æ•…äº‹æ¡†æ¶ä¿æŒä¸€è‡´\n"
                    "2. ç»†èŠ‚æå†™çš„ç”¨è¯å¥‘åˆä¸–ç•Œè§‚è®¾å®š\n"
                    "3. åœ¨ç¬¦åˆè®¾å®šçš„å‰æä¸‹ï¼Œä¼˜åŒ–è¡¨è¾¾è´¨é‡\n"
                )
            elif character_contexts:
                # åªæœ‰äººè®¾
                task_description += (
                    "ã€äººè®¾èµ„æ–™çš„ä½œç”¨ã€‘\n"
                    "äººè®¾èµ„æ–™æ­ç¤ºäº†äººç‰©çš„æ€§æ ¼ã€èƒŒæ™¯ã€è¡Œä¸ºæ¨¡å¼ã€è¯­è¨€ä¹ æƒ¯ç­‰æ ¸å¿ƒç‰¹å¾ã€‚\n"
                    "\n"
                    "è¯·åŸºäºäººè®¾èµ„æ–™å¯¹æ–‡æœ¬è¿›è¡Œæ¶¦è‰²ï¼Œç¡®ä¿ï¼š\n"
                    "1. äººç‰©æ€§æ ¼å’Œè¡Œä¸ºçš„è¡¨è¾¾ç¬¦åˆäººè®¾å®šä½\n"
                    "2. å¯¹è¯å’Œå¿ƒç†æ´»åŠ¨çš„æªè¾ç¬¦åˆè§’è‰²ä¸ªæ€§å’Œè¯­è¨€ä¹ æƒ¯\n"
                    "3. ç»†èŠ‚æå†™çš„ç”¨è¯å¥‘åˆäººç‰©èƒŒæ™¯\n"
                    "4. åœ¨ç¬¦åˆäººè®¾çš„å‰æä¸‹ï¼Œä¼˜åŒ–è¡¨è¾¾è´¨é‡\n"
                )
            
            task_description += (
                "\n"
                "ã€æ¶¦è‰²è¦æ±‚ã€‘\n"
                "1. ä¿æŒåŸæ–‡çš„æ ¸å¿ƒæ„æ€ã€æƒ…èŠ‚å’Œäººç‰©åŠ¨ä½œä¸å˜\n"
                "2. ä¼˜åŒ–ç”¨è¯ã€å¥å¼ã€èŠ‚å¥ï¼Œæå‡æ–‡å­—çš„æµç•…åº¦å’Œå¯è¯»æ€§\n"
                "3. ä¿®æ­£è¯­æ³•é—®é¢˜å’Œä¸å½“è¡¨è¾¾ï¼Œè®©æ–‡å­—æ›´é€šé¡ºè‡ªç„¶\n"
                "4. ç¬¦åˆé£æ ¼è¦æ±‚å’ŒçŸ¥è¯†åº“è®¾å®šï¼Œä¿æŒæ–‡æœ¬çš„æ•´ä½“é£æ ¼ç»Ÿä¸€\n"
                "5. ä»…åšæ¶¦è‰²ä¼˜åŒ–ï¼Œä¸è¦æ·»åŠ æ–°æƒ…èŠ‚æˆ–æ”¹å˜åŸæ„\n"
                "\n"
                "ã€è¾“å‡ºè¦æ±‚ã€‘\n"
                "1. åªè¾“å‡ºæ¶¦è‰²åçš„é‚£ä¸€è¡Œæ–‡æœ¬ï¼Œä¸è¦è¾“å‡ºä¸Šä¸‹æ–‡\n"
                "2. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–æ ‡æ³¨\n"
                "3. ç›´æ¥è¾“å‡ºæ¶¦è‰²åçš„æ–‡æœ¬å†…å®¹å³å¯\n"
            )
            
            system_content += task_description
            
            # æ„å»ºç”¨æˆ·prompt
            context_text = "\n".join(context_lines) if context_lines else "(æ— )"
            
            user_content = "ã€ä¸Šä¸‹æ–‡ã€‘\n" + context_text + "\n\n"
            
            # æ·»åŠ å¤§çº²å‚è€ƒï¼ˆå¦‚æœæœ‰ï¼‰
            if outline_contexts:
                user_content += "ã€å¤§çº²å‚è€ƒèµ„æ–™ã€‘\n"
                user_content += f"ï¼ˆä»å¤§çº²åº“æ‰¾åˆ° {len(outline_contexts)} ä¸ªç›¸å…³è®¾å®šï¼Œç”¨äºç¡®ä¿å‰§æƒ…å‘å±•ç¬¦åˆæ•´ä½“æ¡†æ¶ï¼‰\n\n"
                
                for i, ctx in enumerate(outline_contexts, 1):
                    score = ctx['score']
                    if score >= 0.7:
                        relevance_label = "é«˜åº¦ç›¸å…³"
                    elif score >= 0.5:
                        relevance_label = "è¾ƒä¸ºç›¸å…³"
                    else:
                        relevance_label = "ä¸­ç­‰ç›¸å…³"
                    
                    kb_name = ctx.get('kb_name', '')
                    user_content += f"â•â•â• å¤§çº² {i} ({kb_name}) â•â•â•\n"
                    user_content += f"ç›¸å…³åº¦: {score:.3f} ({relevance_label})\n"
                    user_content += f"ä½œç”¨: å¸®åŠ©ç†è§£æ•…äº‹æ¡†æ¶ã€å‰§æƒ…èµ°å‘å’Œä¸–ç•Œè§‚è®¾å®š\n"
                    user_content += f"å†…å®¹:\n{ctx['full_context']}\n\n"
                
                user_content += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
            
            # æ·»åŠ äººè®¾å‚è€ƒï¼ˆå¦‚æœæœ‰ï¼‰
            if character_contexts:
                user_content += "ã€äººè®¾å‚è€ƒèµ„æ–™ã€‘\n"
                user_content += f"ï¼ˆä»äººè®¾åº“æ‰¾åˆ° {len(character_contexts)} ä¸ªç›¸å…³äººè®¾ï¼Œç”¨äºç¡®ä¿äººç‰©è¡Œä¸ºç¬¦åˆè§’è‰²è®¾å®šï¼‰\n\n"
                
                for i, ctx in enumerate(character_contexts, 1):
                    score = ctx['score']
                    if score >= 0.7:
                        relevance_label = "é«˜åº¦ç›¸å…³"
                    elif score >= 0.5:
                        relevance_label = "è¾ƒä¸ºç›¸å…³"
                    else:
                        relevance_label = "ä¸­ç­‰ç›¸å…³"
                    
                    kb_name = ctx.get('kb_name', '')
                    user_content += f"â•â•â• äººè®¾ {i} ({kb_name}) â•â•â•\n"
                    user_content += f"ç›¸å…³åº¦: {score:.3f} ({relevance_label})\n"
                    user_content += f"ä½œç”¨: å¸®åŠ©ç†è§£è§’è‰²çš„æ€§æ ¼ç‰¹å¾ã€è¡Œä¸ºé€»è¾‘å’Œè¯­è¨€é£æ ¼\n"
                    user_content += f"å†…å®¹:\n{ctx['full_context']}\n\n"
                
                user_content += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
            
            user_content += f"ã€å¾…æ¶¦è‰²æ–‡æœ¬ã€‘\n{target_line}\n\n"
            
            # æ ¹æ®æœ‰å“ªäº›èµ„æ–™è°ƒæ•´æç¤º
            if outline_contexts and character_contexts:
                user_content += "è¯·åŸºäºä¸Šè¿°å¤§çº²å’Œäººè®¾å‚è€ƒåŠä¸Šä¸‹æ–‡ï¼Œè¾“å‡ºæ¶¦è‰²åçš„æ–‡æœ¬ï¼š"
            elif outline_contexts:
                user_content += "è¯·åŸºäºä¸Šè¿°å¤§çº²å‚è€ƒå’Œä¸Šä¸‹æ–‡ï¼Œè¾“å‡ºæ¶¦è‰²åçš„æ–‡æœ¬ï¼š"
            elif character_contexts:
                user_content += "è¯·åŸºäºä¸Šè¿°äººè®¾å‚è€ƒå’Œä¸Šä¸‹æ–‡ï¼Œè¾“å‡ºæ¶¦è‰²åçš„æ–‡æœ¬ï¼š"
            
            # 8. è°ƒç”¨AIè¿›è¡Œæ¶¦è‰²
            payload = {
                "model": self._model,
                "messages": [
                    {
                        "role": "system",
                        "content": system_content,
                    },
                    {
                        "role": "user",
                        "content": user_content,
                    },
                ],
                "temperature": self._temperature,
                "stream": False,
            }
            
            data = self._make_request_with_retry(payload)
            
            # æ‰“å°Tokenä½¿ç”¨ç»Ÿè®¡
            if "usage" in data:
                self._print_token_usage(data["usage"], operation="äººè®¾çŸ¥è¯†åº“å¢å¼ºæ¶¦è‰²")
            
            choices = data.get("choices")
            if not choices:
                raise AIError("AI æœªè¿”å›å†…å®¹ï¼Œè¯·ç¨åå†è¯•ã€‚")
            
            message = choices[0].get("message", {})
            content = message.get("content")
            if not content:
                raise AIError("AI å“åº”å†…å®¹ä¸ºç©ºã€‚")
            
            print(f"[INFO] äººè®¾çŸ¥è¯†åº“å¢å¼ºæ¶¦è‰²å®Œæˆ")
            
            return content.strip()
            
        except Exception as e:
            # å¦‚æœäººè®¾çŸ¥è¯†åº“å¢å¼ºæ¶¦è‰²å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ¶¦è‰²
            print(f"[ERROR] äººè®¾çŸ¥è¯†åº“å¢å¼ºæ¶¦è‰²å¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°æ™®é€šæ¶¦è‰²")
            import traceback
            traceback.print_exc()
            return self.polish_last_line(context_lines, target_line, style_prompt)
    
    def _enhance_query_with_context(self, query_text: str) -> str:
        """æŸ¥è¯¢æ‰©å±•ï¼šæå–å…³é”®ä¿¡æ¯å¢å¼ºæŸ¥è¯¢
        
        Args:
            query_text: åŸå§‹æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            å¢å¼ºåçš„æŸ¥è¯¢æ–‡æœ¬
        """
        # ç®€å•çš„å…³é”®è¯æå–ç­–ç•¥
        # æå–å¯èƒ½çš„äººåã€åœºæ™¯ã€æƒ…èŠ‚å…³é”®è¯
        enhanced_query = query_text
        
        # æå–å¼•å·å†…çš„å¯¹è¯å’Œä¸“æœ‰åè¯ï¼ˆé€šå¸¸æ˜¯å…³é”®ä¿¡æ¯ï¼‰
        import re
        quoted_text = re.findall(r'[ã€Œã€"\'](.*?)[ã€ã€"\']', query_text)
        if quoted_text:
            # å°†å¯¹è¯å†…å®¹æƒé‡æå‡
            enhanced_query = query_text + "\nå…³é”®å¯¹è¯: " + " ".join(quoted_text)
        
        # æå–å¯èƒ½çš„äººåï¼ˆä¸­æ–‡å§“åæ¨¡å¼ï¼š2-4ä¸ªæ±‰å­—ï¼‰
        # åŒ¹é…å¸¸è§å§“æ°å¼€å¤´çš„2-4å­—äººå
        names = re.findall(r'[èµµé’±å­™æå‘¨å´éƒ‘ç‹å†¯é™ˆè¤šå«è’‹æ²ˆéŸ©æ¨æœ±ç§¦å°¤è®¸ä½•å•æ–½å¼ å­”æ›¹ä¸¥åé‡‘é­é™¶å§œ][ä¸€-é¾¥]{1,3}(?=[ï¼Œã€‚ï¼ï¼Ÿ\s"ã€ã€]|$)', query_text)
        if names:
            enhanced_query += "\nç›¸å…³äººç‰©: " + " ".join(set(names))
        
        return enhanced_query
    
    def predict_plot_continuation_with_kb(
        self,
        current_context: str,
        kb_manager: 'KnowledgeBaseManager',
        history_kb: Optional['KnowledgeBase'] = None,
        outline_kb: Optional['KnowledgeBase'] = None,
        character_kb: Optional['KnowledgeBase'] = None,
        rerank_client: Optional['RerankClient'] = None,
        style_prompt: str = "",
        min_relevance_threshold: float = 0.25
    ) -> str:
        """åŸºäºçŸ¥è¯†åº“çš„å¢å¼ºå‰§æƒ…é¢„æµ‹ï¼ˆæ”¯æŒå†å²ã€å¤§çº²ã€äººè®¾ä¸‰åº“æ£€ç´¢ï¼‰
        
        Args:
            current_context: å½“å‰ç¼–è¾‘ä½ç½®çš„ä¸Šä¸‹æ–‡ï¼ˆä¸Šæ–¹ä¸¤è¡Œï¼‰
            kb_manager: çŸ¥è¯†åº“ç®¡ç†å™¨
            history_kb: å†å²æ–‡æœ¬çŸ¥è¯†åº“ï¼ˆå¯é€‰ï¼‰
            outline_kb: å¤§çº²çŸ¥è¯†åº“ï¼ˆå¯é€‰ï¼‰
            character_kb: äººè®¾çŸ¥è¯†åº“ï¼ˆå¯é€‰ï¼‰
            rerank_client: é‡æ’åºå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
            style_prompt: é£æ ¼æç¤ºè¯
            min_relevance_threshold: æœ€å°ç›¸å…³æ€§é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç»“æœä¼šè¢«è¿‡æ»¤ï¼ˆé»˜è®¤0.25ï¼‰
            
        Returns:
            é¢„æµ‹çš„å‰§æƒ…å†…å®¹
        """
        # 1. å¦‚æœä¸‰ä¸ªçŸ¥è¯†åº“éƒ½ä¸ºç©ºï¼Œå›é€€åˆ°æ™®é€šé¢„æµ‹
        if not any([
            history_kb and history_kb.documents,
            outline_kb and outline_kb.documents,
            character_kb and character_kb.documents
        ]):
            print("[INFO] æ‰€æœ‰çŸ¥è¯†åº“éƒ½ä¸ºç©ºï¼Œä½¿ç”¨æ™®é€šé¢„æµ‹")
            return self.predict_plot_continuation(current_context, style_prompt)
        
        # 2. ä½¿ç”¨æŸ¥è¯¢æ‰©å±•å¢å¼ºæ£€ç´¢æ•ˆæœ
        try:
            # æŸ¥è¯¢æ‰©å±•ï¼šæå–å…³é”®ä¿¡æ¯å¢å¼ºæŸ¥è¯¢
            enhanced_query = self._enhance_query_with_context(current_context)
            if enhanced_query != current_context:
                print(f"[INFO] æŸ¥è¯¢æ‰©å±•å·²å¯ç”¨ï¼ŒåŸå§‹æŸ¥è¯¢é•¿åº¦: {len(current_context)}, å¢å¼ºå: {len(enhanced_query)}")
            
            # 3. åˆ†åˆ«ä»ä¸‰ä¸ªçŸ¥è¯†åº“æ£€ç´¢ï¼ˆæ¯ä¸ªæœ€å¤š5æ¡ï¼‰
            history_docs = []
            outline_docs = []
            character_docs = []
            
            # ä»å†å²æ–‡æœ¬çŸ¥è¯†åº“æ£€ç´¢
            if history_kb and history_kb.documents:
                print(f"[INFO] å¼€å§‹å†å²æ–‡æœ¬çŸ¥è¯†åº“æ£€ç´¢ï¼Œæ–‡æ¡£æ•°: {len(history_kb.documents)}")
                history_docs = kb_manager.search_similar_documents(
                    query_text=enhanced_query,
                    kb=history_kb,
                    top_k=25,
                    rerank_client=rerank_client,
                    final_top_n=5
                )
                print(f"[INFO] å†å²æ–‡æœ¬æ£€ç´¢ï¼šæ‰¾åˆ° {len(history_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # ä»å¤§çº²çŸ¥è¯†åº“æ£€ç´¢
            if outline_kb and outline_kb.documents:
                print(f"[INFO] å¼€å§‹å¤§çº²çŸ¥è¯†åº“æ£€ç´¢ï¼Œæ–‡æ¡£æ•°: {len(outline_kb.documents)}")
                outline_docs = kb_manager.search_similar_documents(
                    query_text=enhanced_query,
                    kb=outline_kb,
                    top_k=20,
                    rerank_client=rerank_client,
                    final_top_n=5
                )
                print(f"[INFO] å¤§çº²æ£€ç´¢ï¼šæ‰¾åˆ° {len(outline_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # ä»äººè®¾çŸ¥è¯†åº“æ£€ç´¢
            if character_kb and character_kb.documents:
                print(f"[INFO] å¼€å§‹äººè®¾çŸ¥è¯†åº“æ£€ç´¢ï¼Œæ–‡æ¡£æ•°: {len(character_kb.documents)}")
                character_docs = kb_manager.search_similar_documents(
                    query_text=enhanced_query,
                    kb=character_kb,
                    top_k=20,
                    rerank_client=rerank_client,
                    final_top_n=5
                )
                print(f"[INFO] äººè®¾æ£€ç´¢ï¼šæ‰¾åˆ° {len(character_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # åˆå¹¶æ£€ç´¢ç»“æœ
            similar_docs = history_docs + outline_docs + character_docs
            
            # 3. æ ¹æ®åŠ¨æ€é˜ˆå€¼è¿‡æ»¤ä½è´¨é‡ç»“æœï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            filtered_docs = []
            
            if similar_docs:
                # åŠ¨æ€é˜ˆå€¼ç­–ç•¥ï¼šåŸºäºæœ€é«˜åˆ†æ•°è°ƒæ•´é˜ˆå€¼
                max_score = similar_docs[0].get('relevance_score', similar_docs[0].get('similarity_score', 0))
                
                # å¦‚æœæœ€é«˜åˆ†æ•°å¾ˆé«˜(>=0.7)ï¼Œä½¿ç”¨ç›¸å¯¹é˜ˆå€¼ï¼ˆæœ€é«˜åˆ†çš„40%ï¼‰
                # å¦‚æœæœ€é«˜åˆ†æ•°ä¸­ç­‰(0.4-0.7)ï¼Œä½¿ç”¨è¾ƒä½çš„ç›¸å¯¹é˜ˆå€¼ï¼ˆæœ€é«˜åˆ†çš„30%ï¼‰
                # å¦‚æœæœ€é«˜åˆ†æ•°è¾ƒä½(<0.4)ï¼Œä½¿ç”¨ç»å¯¹æœ€ä½é˜ˆå€¼
                if max_score >= 0.7:
                    dynamic_threshold = max(min_relevance_threshold, max_score * 0.4)
                elif max_score >= 0.4:
                    dynamic_threshold = max(min_relevance_threshold, max_score * 0.3)
                else:
                    dynamic_threshold = min_relevance_threshold
                
                print(f"[INFO] åŠ¨æ€é˜ˆå€¼è®¡ç®—ï¼šæœ€é«˜åˆ†={max_score:.3f}ï¼ŒåŠ¨æ€é˜ˆå€¼={dynamic_threshold:.3f}ï¼ŒåŸºç¡€é˜ˆå€¼={min_relevance_threshold:.3f}")
                
                for doc_item in similar_docs:
                    # å¦‚æœæœ‰é‡æ’åˆ†æ•°ï¼Œä½¿ç”¨é‡æ’åˆ†æ•°ï¼›å¦åˆ™ä½¿ç”¨ç›¸ä¼¼åº¦åˆ†æ•°
                    score = doc_item.get('relevance_score', doc_item.get('similarity_score', 0))
                    
                    if score >= dynamic_threshold:
                        filtered_docs.append(doc_item)
                
                # å¦‚æœè¿‡æ»¤åæ²¡æœ‰ç»“æœï¼Œè‡³å°‘ä¿ç•™ç›¸å…³æ€§æœ€é«˜çš„1-2ä¸ªæ–‡æ¡£
                if not filtered_docs:
                    filtered_docs = similar_docs[:min(2, len(similar_docs))]
                    print(f"[INFO] çŸ¥è¯†åº“æ£€ç´¢ï¼šæ‰€æœ‰æ–‡æ¡£ç›¸å…³æ€§ä½äºé˜ˆå€¼ï¼Œä¿ç•™æœ€é«˜çš„ {len(filtered_docs)} ä¸ª")
                
                # ä¸éœ€è¦é™åˆ¶æ€»æ•°ï¼Œå› ä¸ºæ¯ä¸ªçŸ¥è¯†åº“å·²ç»é™åˆ¶ä¸ºæœ€å¤š5æ¡
            
            print(f"[INFO] çŸ¥è¯†åº“æ£€ç´¢ï¼šæ‰¾åˆ° {len(similar_docs)} ä¸ªç›¸ä¼¼æ–‡æ¡£ï¼Œè¿‡æ»¤åä¿ç•™ {len(filtered_docs)} ä¸ª")
            
            # 4. å¦‚æœç¡®å®æ²¡æœ‰ç»“æœï¼ˆçŸ¥è¯†åº“ä¸ºç©ºï¼‰ï¼Œå›é€€åˆ°æ™®é€šé¢„æµ‹
            if not filtered_docs:
                print("[INFO] æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œä½¿ç”¨æ™®é€šé¢„æµ‹")
                return self.predict_plot_continuation(current_context, style_prompt)
            
            # 5. æå–æ¯ä¸ªæ–‡æ¡£çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶æ ‡è®°æ¥æºï¼ˆä¸‰ç§çŸ¥è¯†åº“åˆ†åˆ«å¤„ç†ï¼‰
            history_contexts = []
            outline_contexts = []
            character_contexts = []
            
            for doc_item in filtered_docs:
                doc = doc_item['document']
                
                # åˆ¤æ–­æ–‡æ¡£æ¥æºï¼ˆé€šè¿‡doc_idåœ¨å“ªä¸ªçŸ¥è¯†åº“ä¸­ï¼‰
                is_from_history = False
                is_from_outline = False
                is_from_character = False
                
                if history_kb and history_kb.documents:
                    if any(d.id == doc.id for d in history_kb.documents):
                        is_from_history = True
                
                if outline_kb and outline_kb.documents:
                    if any(d.id == doc.id for d in outline_kb.documents):
                        is_from_outline = True
                
                if character_kb and character_kb.documents:
                    if any(d.id == doc.id for d in character_kb.documents):
                        is_from_character = True
                
                # è·å–æ–‡æ¡£åŠå…¶ä¸Šä¸‹æ–‡
                if is_from_history:
                    doc_with_context = kb_manager.get_document_with_context(
                        doc=doc,
                        kb=history_kb,
                        context_lines_before=4,
                        context_lines_after=4
                    )
                    history_contexts.append({
                        'content': doc.content,
                        'full_context': doc_with_context['full_context'],
                        'file_path': doc_with_context['file_path'],
                        'score': doc_item.get('relevance_score', doc_item.get('similarity_score', 0))
                    })
                elif is_from_outline:
                    doc_with_context = kb_manager.get_document_with_context(
                        doc=doc,
                        kb=outline_kb,
                        context_lines_before=2,
                        context_lines_after=2
                    )
                    outline_contexts.append({
                        'content': doc.content,
                        'full_context': doc_with_context['full_context'],
                        'file_path': doc_with_context['file_path'],
                        'score': doc_item.get('relevance_score', doc_item.get('similarity_score', 0))
                    })
                elif is_from_character:
                    doc_with_context = kb_manager.get_document_with_context(
                        doc=doc,
                        kb=character_kb,
                        context_lines_before=2,
                        context_lines_after=2
                    )
                    character_contexts.append({
                        'content': doc.content,
                        'full_context': doc_with_context['full_context'],
                        'file_path': doc_with_context['file_path'],
                        'score': doc_item.get('relevance_score', doc_item.get('similarity_score', 0))
                    })
            
            # 6. æ„å»ºçŸ¥è¯†åº“å¢å¼ºé¢„æµ‹prompt - æ˜ç¡®æ ‡æ³¨ä¸º"å‰§æƒ…é¢„æµ‹"ä»»åŠ¡
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_content = (
                "ä½ æ˜¯ä¸€ä½å¯Œæœ‰åˆ›é€ åŠ›çš„èµ„æ·±ä¸­æ–‡å°è¯´ä½œå®¶ã€‚\n"
                "\n"
                "ã€ä½ çš„ä»»åŠ¡ã€‘åŸºäºçŸ¥è¯†åº“çš„å‰§æƒ…é¢„æµ‹ä¸åˆ›ä½œ\n"
                "ç»“åˆå½“å‰ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†åº“å‚è€ƒèµ„æ–™ï¼Œç”Ÿæˆæ¥ä¸‹æ¥çš„æ•…äº‹å†…å®¹ã€‚\n"
                "è¿™ä¸æ˜¯æ¶¦è‰²ä»»åŠ¡ï¼Œè€Œæ˜¯åˆ›é€ æ€§çš„ç»­å†™å’Œå‰§æƒ…å‘å±•ã€‚\n"
                "\n"
                "ã€æ ¸å¿ƒåŸåˆ™ï¼šæƒé‡ä¼˜å…ˆçº§ã€‘\n"
                "ğŸ”´ æœ€é«˜ä¼˜å…ˆçº§ï¼šã€å½“å‰ä¸Šä¸‹æ–‡ã€‘- è¿™æ˜¯ç”¨æˆ·æ­£åœ¨ä¹¦å†™çš„å®é™…å†…å®¹ï¼Œæ˜¯å‰§æƒ…å‘å±•çš„æ ¸å¿ƒä¾æ®\n"
                "ğŸŸ¡ è¾…åŠ©å‚è€ƒï¼šçŸ¥è¯†åº“èµ„æ–™ - æä¾›èƒŒæ™¯è®¾å®šå’Œåˆ›æ„çµæ„Ÿï¼Œä½†å¿…é¡»æœä»å½“å‰ä¸Šä¸‹æ–‡\n"
                "\n"
                "âš ï¸ å…³é”®è¦æ±‚ï¼š\n"
                "â€¢ å½“å‰ä¸Šä¸‹æ–‡æ˜¯æœ€é‡è¦çš„ï¼Œå¿…é¡»ç´§å¯†åŸºäºå®ƒæ¥é¢„æµ‹åç»­å‰§æƒ…\n"
                "â€¢ çŸ¥è¯†åº“èµ„æ–™ä»…ä½œä¸ºè¾…åŠ©ï¼Œç”¨äºç†è§£èƒŒæ™¯ã€äººè®¾ã€ä¸–ç•Œè§‚\n"
                "â€¢ å¦‚æœçŸ¥è¯†åº“å‚è€ƒä¸å½“å‰ä¸Šä¸‹æ–‡å†²çªï¼Œå¿…é¡»ä»¥å½“å‰ä¸Šä¸‹æ–‡ä¸ºå‡†\n"
                "â€¢ é¢„æµ‹å¿…é¡»æ— ç¼æ¥ç»­å½“å‰ä¸Šä¸‹æ–‡çš„æœ«å°¾ï¼Œä¸èƒ½è„±ç¦»å½“å‰æƒ…å¢ƒ\n"
                "\n"
                "ã€å¦‚ä½•ä½¿ç”¨çŸ¥è¯†åº“å‚è€ƒï¼ˆè¾…åŠ©æ€§è´¨ï¼‰ã€‘\n"
                "âœ¦ å‚è€ƒå†…å®¹çš„ä»·å€¼ï¼š\n"
                "  â€¢ æ­ç¤ºäººç‰©æ€§æ ¼æ·±å±‚é€»è¾‘ã€æƒ…èŠ‚è½¬æŠ˜è§„å¾‹ã€æ½œåœ¨ä¼ç¬”\n"
                "  â€¢ å±•ç°ä½œè€…åå¥½çš„å™äº‹æŠ€å·§ã€æˆå‰§å†²çªæ¨¡å¼ã€æƒ…æ„Ÿè¡¨è¾¾æ–¹å¼\n"
                "  â€¢ æä¾›å¯å€Ÿé‰´çš„åˆ›æ„å…ƒç´ ã€æ„å¤–è½¬æŠ˜ã€äººç‰©å…³ç³»å¼ åŠ›\n"
                "âœ¦ åˆ›æ„è¿ç”¨ç­–ç•¥ï¼š\n"
                "  1. åœ¨ä¸åç¦»å½“å‰ä¸Šä¸‹æ–‡çš„å‰æä¸‹ï¼Œå€Ÿé‰´å‚è€ƒä¸­çš„æƒ…èŠ‚æ¨¡å¼\n"
                "  2. å‘ç°å‚è€ƒä¸­çš„ä¼ç¬”çº¿ç´¢ï¼Œåœ¨ç¬¦åˆå½“å‰æƒ…å¢ƒæ—¶å·§å¦™å‘¼åº”\n"
                "  3. å­¦ä¹ å‚è€ƒä¸­åˆ¶é€ æ‚¬å¿µã€åè½¬ã€å†²çªçš„æŠ€å·§\n"
                "  4. ä»å‚è€ƒä¸­ç†è§£äººç‰©çš„æ ¸å¿ƒåŠ¨æœºå’Œè¡Œä¸ºé€»è¾‘\n"
                "âœ¦ æ³¨æ„äº‹é¡¹ï¼š\n"
                "  Ã— ä¸è¦æœºæ¢°å¤åˆ¶å‚è€ƒå†…å®¹ï¼Œè¦åˆ›é€ æ€§è½¬åŒ–\n"
                "  Ã— å‚è€ƒåªæ˜¯çµæ„Ÿæ¥æºï¼Œå½“å‰ä¸Šä¸‹æ–‡æ‰æ˜¯åˆ›ä½œåŸºç¡€\n"
                "  Ã— é¿å…å¹³åº¸ç»­å†™ï¼Œä½†ä¹Ÿä¸èƒ½è„±ç¦»å½“å‰å‰§æƒ…"
            )
            
            # æ·»åŠ é£æ ¼è¦æ±‚
            if style_prompt:
                system_content += f"\n\nã€ä½ çš„å†™ä½œé£æ ¼ã€‘\n{style_prompt}"
            
            # æ·»åŠ é¢„æµ‹ä»»åŠ¡è¦æ±‚
            system_content += (
                "\n\nã€å‰§æƒ…é¢„æµ‹è¦æ±‚ï¼ˆæƒé‡æ’åºï¼‰ã€‘\n"
                "1. ğŸ”´ ç´§æ‰£å½“å‰ä¸Šä¸‹æ–‡ï¼šå¿…é¡»åŸºäºã€å½“å‰ä¸Šä¸‹æ–‡ã€‘çš„å…·ä½“æƒ…å¢ƒã€äººç‰©çŠ¶æ€ã€åœºæ™¯ç»†èŠ‚æ¥é¢„æµ‹\n"
                "2. ğŸŸ¡ å‚è€ƒçŸ¥è¯†åº“ï¼šåœ¨ç†è§£å½“å‰æƒ…å¢ƒçš„åŸºç¡€ä¸Šï¼Œå€Ÿé‰´çŸ¥è¯†åº“ä¸­çš„äººè®¾ã€å¤§çº²ã€å†å²å‰§æƒ…\n"
                "3. ğŸ­ åˆ›æ„å‘å±•ï¼šä»å½“å‰æƒ…å¢ƒå‡ºå‘ï¼Œé€‰æ‹©æœ€æœ‰æˆå‰§å¼ åŠ›å’Œæƒ…æ„Ÿå†²å‡»çš„å‘å±•æ–¹å‘\n"
                "4. âœ… é€»è¾‘åˆç†ï¼šç¡®ä¿é¢„æµ‹æ—¢æ–°é¢–åˆç¬¦åˆå½“å‰å·²å»ºç«‹çš„é€»è¾‘å’Œæƒ…å¢ƒ\n"
                "5. ğŸ¨ é£æ ¼å¥‘åˆï¼šç”¨ç¬¦åˆä¸Šè¿°å†™ä½œé£æ ¼çš„è¯­è¨€è¡¨è¾¾\n"
                "6. ğŸ”— æ— ç¼è¡”æ¥ï¼šè¾“å‡ºå¿…é¡»ç›´æ¥æ¥ç»­å½“å‰ä¸Šä¸‹æ–‡çš„æœ«å°¾ï¼Œä¸èƒ½è·³è·ƒæˆ–è„±èŠ‚\n"
                "\n"
                "ã€è¾“å‡ºè¦æ±‚ã€‘\n"
                "- åªè¾“å‡ºä¸¤è¡Œçº¯æ–‡æœ¬ï¼ˆæ¯è¡Œä¸€ä¸ªå®Œæ•´å¥å­ï¼‰\n"
                "- ä¸è¦ä»»ä½•è§£é‡Šã€æ ‡æ³¨æˆ–å…ƒæ•°æ®\n"
                "- ç›´æ¥è¾“å‡ºé¢„æµ‹çš„åç»­å‰§æƒ…"
            )
            
            # æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆç»“æ„åŒ–å‘ˆç°ä¸‰ç§çŸ¥è¯†åº“ï¼Œå¹¶æ ‡æ³¨å„è‡ªä½œç”¨ï¼‰
            user_content = "ã€å½“å‰ä¸Šä¸‹æ–‡ã€‘\n" + current_context + "\n\n"
            
            # æ·»åŠ å†å²æ–‡æœ¬å‚è€ƒï¼ˆå¦‚æœæœ‰ï¼‰
            if history_contexts:
                user_content += "ã€å†å²å‰§æƒ…å‚è€ƒã€‘\n"
                user_content += f"ï¼ˆä»å†å²æ–‡æœ¬åº“æ‰¾åˆ° {len(history_contexts)} ä¸ªç›¸å…³ç‰‡æ®µï¼Œæœ€å¤š5æ¡ï¼‰\n"
                user_content += "ä½œç”¨: æä¾›ç›¸ä¼¼æƒ…èŠ‚çš„å†™ä½œé£æ ¼ã€å‰§æƒ…å‘å±•æ¨¡å¼å’Œåˆ›æ„çµæ„Ÿ\n\n"
                
                for i, ctx in enumerate(history_contexts, 1):
                    score = ctx['score']
                    if score >= 0.7:
                        relevance_label = "é«˜åº¦ç›¸å…³"
                    elif score >= 0.5:
                        relevance_label = "è¾ƒä¸ºç›¸å…³"
                    elif score >= 0.3:
                        relevance_label = "ä¸­ç­‰ç›¸å…³"
                    else:
                        relevance_label = "å¼±ç›¸å…³"
                    
                    user_content += f"â•â•â• å†å²ç‰‡æ®µ {i} â•â•â•\n"
                    user_content += f"ç›¸å…³åº¦: {score:.3f} ({relevance_label})\n"
                    user_content += f"å†…å®¹:\n{ctx['full_context']}\n\n"
                
                user_content += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
            
            # æ·»åŠ å¤§çº²å‚è€ƒï¼ˆå¦‚æœæœ‰ï¼‰
            if outline_contexts:
                user_content += "ã€å¤§çº²å‚è€ƒèµ„æ–™ã€‘\n"
                user_content += f"ï¼ˆä»å¤§çº²åº“æ‰¾åˆ° {len(outline_contexts)} ä¸ªç›¸å…³è®¾å®šï¼Œæœ€å¤š5æ¡ï¼‰\n"
                user_content += "ä½œç”¨: ç¡®ä¿å‰§æƒ…å‘å±•ç¬¦åˆæ•´ä½“è§„åˆ’å’Œä¸–ç•Œè§‚è®¾å®š\n\n"
                
                for i, ctx in enumerate(outline_contexts, 1):
                    score = ctx['score']
                    if score >= 0.7:
                        relevance_label = "é«˜åº¦ç›¸å…³"
                    elif score >= 0.5:
                        relevance_label = "è¾ƒä¸ºç›¸å…³"
                    else:
                        relevance_label = "ä¸­ç­‰ç›¸å…³"
                    
                    user_content += f"â•â•â• å¤§çº² {i} â•â•â•\n"
                    user_content += f"ç›¸å…³åº¦: {score:.3f} ({relevance_label})\n"
                    user_content += f"å†…å®¹:\n{ctx['full_context']}\n\n"
                
                user_content += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
            
            # æ·»åŠ äººè®¾å‚è€ƒï¼ˆå¦‚æœæœ‰ï¼‰
            if character_contexts:
                user_content += "ã€äººè®¾å‚è€ƒèµ„æ–™ã€‘\n"
                user_content += f"ï¼ˆä»äººè®¾åº“æ‰¾åˆ° {len(character_contexts)} ä¸ªç›¸å…³äººè®¾ï¼Œæœ€å¤š5æ¡ï¼‰\n"
                user_content += "ä½œç”¨: ç¡®ä¿è§’è‰²è¡Œä¸ºã€å¯¹è¯å’Œå¿ƒç†æ´»åŠ¨ç¬¦åˆäººç‰©è®¾å®š\n\n"
                
                for i, ctx in enumerate(character_contexts, 1):
                    score = ctx['score']
                    if score >= 0.7:
                        relevance_label = "é«˜åº¦ç›¸å…³"
                    elif score >= 0.5:
                        relevance_label = "è¾ƒä¸ºç›¸å…³"
                    else:
                        relevance_label = "ä¸­ç­‰ç›¸å…³"
                    
                    user_content += f"â•â•â• äººè®¾ {i} â•â•â•\n"
                    user_content += f"ç›¸å…³åº¦: {score:.3f} ({relevance_label})\n"
                    user_content += f"å†…å®¹:\n{ctx['full_context']}\n\n"
                
                user_content += "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
            
            # æ·»åŠ ä»»åŠ¡è¯´æ˜ - å¼ºè°ƒå½“å‰ä¸Šä¸‹æ–‡çš„ä¼˜å…ˆçº§
            user_content += "ã€é¢„æµ‹æŒ‡ä»¤ã€‘\n"
            user_content += "ğŸ”´ æ ¸å¿ƒä¾æ®ï¼šè¯·ç´§å¯†åŸºäºã€å½“å‰ä¸Šä¸‹æ–‡ã€‘çš„å…·ä½“æƒ…å¢ƒæ¥é¢„æµ‹åç»­å‰§æƒ…\n"
            
            refs = []
            if history_contexts:
                refs.append("ã€å†å²å‰§æƒ…å‚è€ƒã€‘")
            if outline_contexts:
                refs.append("ã€å¤§çº²å‚è€ƒèµ„æ–™ã€‘")
            if character_contexts:
                refs.append("ã€äººè®¾å‚è€ƒèµ„æ–™ã€‘")
            
            if refs:
                user_content += f"ğŸŸ¡ è¾…åŠ©å‚è€ƒï¼š{' '.join(refs)}å¯ä½œä¸ºèƒŒæ™¯ç†è§£å’Œåˆ›æ„çµæ„Ÿ\n"
            
            user_content += "\né‡è¦æé†’ï¼š\n"
            user_content += "â€¢ ã€å½“å‰ä¸Šä¸‹æ–‡ã€‘æ˜¯æœ€é‡è¦çš„ï¼Œé¢„æµ‹å¿…é¡»ä»å®ƒå‡ºå‘\n"
            user_content += "â€¢ çŸ¥è¯†åº“å‚è€ƒä»…ä½œä¸ºè¾…åŠ©ï¼Œä¸èƒ½åç¦»å½“å‰æƒ…å¢ƒ\n"
            user_content += "â€¢ å¦‚æœå‚è€ƒä¸å½“å‰å†²çªï¼Œä»¥å½“å‰ä¸Šä¸‹æ–‡ä¸ºå‡†\n\n"
            user_content += "ç°åœ¨ï¼Œè¯·åŸºäºã€å½“å‰ä¸Šä¸‹æ–‡ã€‘ç”Ÿæˆä»¤äººçœ¼å‰ä¸€äº®ã€åˆåœ¨æƒ…ç†ä¹‹ä¸­çš„åç»­ä¸¤è¡Œå‰§æƒ…ï¼š\n"
            user_content += "ï¼ˆç›´æ¥è¾“å‡ºä¸¤è¡Œæ–‡æœ¬ï¼Œä¸è¦ä»»ä½•å…¶ä»–å†…å®¹ï¼‰"
            
            # 7. è°ƒç”¨AIç”Ÿæˆé¢„æµ‹
            payload = {
                "model": self._model,
                "messages": [
                    {
                        "role": "system",
                        "content": system_content,
                    },
                    {
                        "role": "user",
                        "content": user_content
                    },
                ],
                "temperature": 0.8,  # ä½¿ç”¨è¾ƒé«˜æ¸©åº¦ä»¥å¢åŠ åˆ›é€ æ€§å’Œæ„å¤–æ€§
                "stream": False,
            }
            
            # ä½¿ç”¨ä¼˜åŒ–çš„è¯·æ±‚æ–¹æ³•ï¼ˆåŒ…å«é‡è¯•å’Œé”™è¯¯å¤„ç†ï¼‰
            data = self._make_request_with_retry(payload)
            
            # æ‰“å°Tokenä½¿ç”¨ç»Ÿè®¡ï¼ˆçŸ¥è¯†åº“å¢å¼ºå‰§æƒ…é¢„æµ‹ï¼‰
            if "usage" in data:
                self._print_token_usage(data["usage"], operation="çŸ¥è¯†åº“å¢å¼ºå‰§æƒ…é¢„æµ‹")
            
            choices = data.get("choices")
            if not choices:
                raise AIError("AI æœªè¿”å›å†…å®¹ï¼Œè¯·ç¨åå†è¯•ã€‚")
            
            message = choices[0].get("message", {})
            content = message.get("content")
            if not content:
                raise AIError("AI å“åº”å†…å®¹ä¸ºç©ºã€‚")
            
            return content.strip()
            
        except Exception as e:
            # å¦‚æœçŸ¥è¯†åº“æ£€ç´¢æˆ–é¢„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šé¢„æµ‹
            print(f"[ERROR] çŸ¥è¯†åº“å¢å¼ºé¢„æµ‹å¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°æ™®é€šé¢„æµ‹")
            import traceback
            traceback.print_exc()
            return self.predict_plot_continuation(current_context, style_prompt)
    
    def batch_polish_document(self, content: str, requirement: str = "") -> str:
        """æ‰¹é‡æ¶¦è‰²æ•´ä¸ªæ–‡æ¡£
        
        Args:
            content: è¦æ¶¦è‰²çš„å®Œæ•´æ–‡æ¡£å†…å®¹
            requirement: ç”¨æˆ·çš„æ¶¦è‰²éœ€æ±‚ï¼ˆä¾‹å¦‚ï¼šæå‡ä¸“ä¸šæ€§ã€å£è¯­åŒ–ç­‰ï¼‰
        
        Returns:
            æ¶¦è‰²åçš„æ–‡æ¡£å†…å®¹
        """
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_content = "ä½ æ˜¯ä¸€ä½èµ„æ·±ä¸­æ–‡æ–‡æ¡£ç¼–è¾‘å’Œæ¶¦è‰²ä¸“å®¶ã€‚"
        
        # å¦‚æœæœ‰ç”¨æˆ·éœ€æ±‚ï¼ŒåŠ å…¥éœ€æ±‚è¯´æ˜
        if requirement:
            system_content += f"\n\nã€æ¶¦è‰²éœ€æ±‚ã€‘\n{requirement}"
        
        # æ·»åŠ æ ¸å¿ƒä»»åŠ¡æŒ‡ä»¤ - çº¯ç²¹çš„æ¶¦è‰²ï¼Œä¸åšåˆ›é€ æ€§æ”¹å†™
        system_content += (
            "\n\nã€æ ¸å¿ƒä»»åŠ¡ã€‘"
            "\nå¯¹æ•´ä¸ªæ–‡æ¡£è¿›è¡Œæ¶¦è‰²ä¼˜åŒ–ï¼Œä¿æŒåŸæ„å’Œæƒ…èŠ‚ï¼Œæå‡è¡¨è¾¾è´¨é‡ã€‚"
            "\n\nã€æ¶¦è‰²è¦æ±‚ã€‘"
            "\n1. ä¿æŒåŸæ–‡çš„æ ¸å¿ƒæ„æ€ã€æƒ…èŠ‚ç»“æ„å’Œæ®µè½ç»„ç»‡ä¸å˜"
            "\n2. ä¼˜åŒ–ç”¨è¯ã€å¥å¼ã€èŠ‚å¥ï¼Œæå‡æ–‡å­—çš„æµç•…åº¦å’Œå¯è¯»æ€§"
            "\n3. ä¿®æ­£è¯­æ³•é”™è¯¯å’Œä¸å½“è¡¨è¾¾ï¼Œè®©æ–‡å­—æ›´é€šé¡ºè‡ªç„¶"
            "\n4. æ ¹æ®ç”¨æˆ·éœ€æ±‚è°ƒæ•´æ–‡æœ¬é£æ ¼ï¼Œä¿æŒæ•´ä½“é£æ ¼ç»Ÿä¸€"
            "\n5. ä¿æŒåŸæ–‡çš„æ®µè½æ ¼å¼å’Œæ¢è¡Œç»“æ„"
            "\n6. ä»…åšæ¶¦è‰²ä¼˜åŒ–ï¼Œä¸è¦æ·»åŠ æ–°æƒ…èŠ‚æˆ–æ”¹å˜åŸæ„"
            "\n\nã€è¾“å‡ºè¦æ±‚ã€‘"
            "\n- ç›´æ¥è¾“å‡ºæ¶¦è‰²åçš„å®Œæ•´æ–‡æ¡£å†…å®¹"
            "\n- ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€è¯„è®ºæˆ–é¢å¤–è¯´æ˜"
            "\n- ä¿æŒåŸæ–‡çš„æ®µè½ç»“æ„å’Œæ¢è¡Œ"
            "\n- åªè¾“å‡ºæ¶¦è‰²åçš„æ–‡æœ¬"
        )
        
        payload = {
            "model": self._model,
            "messages": [
                {
                    "role": "system",
                    "content": system_content
                },
                {
                    "role": "user",
                    "content": content
                }
            ],
            "temperature": self._temperature,
            "stream": False
        }
        
        # æ‰¹é‡æ¶¦è‰²ä½¿ç”¨æ›´é•¿è¶…æ—¶ï¼ˆ2å€è¶…æ—¶ï¼‰
        data = self._make_request_with_retry(payload, timeout=self._timeout_seconds * 2)
        
        # æ‰“å°Tokenä½¿ç”¨ç»Ÿè®¡
        if "usage" in data:
            self._print_token_usage(data["usage"], operation="æ‰¹é‡æ¶¦è‰²")
        
        choices = data.get("choices")
        if not choices:
            raise AIError("AI æœªè¿”å›å†…å®¹ï¼Œè¯·ç¨åå†è¯•ã€‚")
        
        message = choices[0].get("message", {})
        content = message.get("content")
        
        if not content:
            raise AIError("AI å“åº”å†…å®¹ä¸ºç©ºã€‚")
        
        print(f"[DEBUG API] batch_polish_document å®Œæˆï¼Œè¿”å›å†…å®¹é•¿åº¦: {len(content)}", flush=True)
        
        return content.strip()
    
    def check_connection_alive(self) -> bool:
        """è½»é‡çº§è¿æ¥æ£€æŸ¥"""
        return hasattr(self, '_session') and self._session is not None
    
    def test_connection(self) -> Dict[str, Any]:
        """æµ‹è¯•APIè¿æ¥"""
        try:
            test_payload = {
                "model": self._model,
                "messages": [{"role": "user", "content": "test"}],
                "temperature": 0.1,
                "max_tokens": 5,
                "stream": False,
            }
            
            response = self._session.post(
                self._base_url,
                headers=self._build_headers(),
                json=test_payload,
                timeout=10,
            )
            
            return {
                "success": response.status_code == 200,
                "message": "è¿æ¥æ­£å¸¸" if response.status_code == 200 else f"é”™è¯¯: {response.status_code}",
                "status_code": response.status_code
            }
        except Exception as e:
            return {
                "success": False,
                "message": str(e),
                "status_code": None
            }
    
    def warmup_connection(self) -> Dict[str, Any]:
        """è½»é‡çº§é¢„çƒ­ - Sessionè‡ªåŠ¨ç®¡ç†è¿æ¥ï¼Œæ— éœ€é¢å¤–æ“ä½œ"""
        return {
            "success": True,
            "message": "å°±ç»ª",
            "warmup_time": 0.0
        }
    
    def is_warmed_up(self) -> bool:
        """æ£€æŸ¥è¿æ¥æ˜¯å¦å·²é¢„çƒ­"""
        return True  # Sessionå§‹ç»ˆå°±ç»ª
    
    def close(self):
        """å…³é—­è¿æ¥æ± ï¼Œé‡Šæ”¾èµ„æº"""
        try:
            if hasattr(self, '_session') and self._session:
                self._session.close()
        except Exception:
            pass  # å¿½ç•¥å…³é—­æ—¶çš„é”™è¯¯
    
    def __del__(self):
        """ææ„å‡½æ•° - ç¡®ä¿èµ„æºè¢«é‡Šæ”¾"""
        self.close()
